{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7d93fda5-9dce-4f5a-8dae-f60809a6811b",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# XGBoost Benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "75011087-5a5b-4743-9e88-d84dd49e3119",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[43mNote: you may need to restart the kernel using dbutils.library.restartPython() to use updated packages.\u001B[0m\nLooking in indexes: https://pypi.org/simple, https://pypi.nvidia.com\nCollecting cudf-cu11\n  Downloading https://pypi.nvidia.com/cudf-cu11/cudf_cu11-23.6.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (489.3 MB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 489.3/489.3 MB 1.3 MB/s eta 0:00:00\nRequirement already satisfied: typing-extensions>=4.0.0 in /databricks/python3/lib/python3.10/site-packages (from cudf-cu11) (4.3.0)\nCollecting ptxcompiler-cu11\n  Downloading https://pypi.nvidia.com/ptxcompiler-cu11/ptxcompiler_cu11-0.7.0.post1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (8.8 MB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 8.8/8.8 MB 79.3 MB/s eta 0:00:00\nCollecting cubinlinker-cu11\n  Downloading https://pypi.nvidia.com/cubinlinker-cu11/cubinlinker_cu11-0.3.0.post1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (8.8 MB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 8.8/8.8 MB 74.2 MB/s eta 0:00:00\nCollecting protobuf<4.22,>=4.21.6\n  Downloading protobuf-4.21.12-cp37-abi3-manylinux2014_x86_64.whl (409 kB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 409.8/409.8 kB 6.9 MB/s eta 0:00:00\nRequirement already satisfied: pandas<1.6.0dev0,>=1.3 in /databricks/python3/lib/python3.10/site-packages (from cudf-cu11) (1.4.4)\nCollecting cupy-cuda11x>=12.0.0\n  Downloading cupy_cuda11x-12.1.0-cp310-cp310-manylinux2014_x86_64.whl (89.3 MB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 89.3/89.3 MB 20.9 MB/s eta 0:00:00\nCollecting pyarrow==11.*\n  Downloading pyarrow-11.0.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (34.9 MB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 34.9/34.9 MB 39.6 MB/s eta 0:00:00\nRequirement already satisfied: numpy>=1.21 in /databricks/python3/lib/python3.10/site-packages (from cudf-cu11) (1.21.5)\nCollecting nvtx>=0.2.1\n  Downloading nvtx-0.2.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (553 kB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 553.1/553.1 kB 56.3 MB/s eta 0:00:00\nRequirement already satisfied: packaging in /databricks/python3/lib/python3.10/site-packages (from cudf-cu11) (21.3)\nCollecting numba>=0.57\n  Downloading numba-0.57.1-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (3.6 MB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 3.6/3.6 MB 44.7 MB/s eta 0:00:00\nRequirement already satisfied: cachetools in /databricks/python3/lib/python3.10/site-packages (from cudf-cu11) (4.2.4)\nCollecting cuda-python<12.0,>=11.7.1\n  Downloading cuda_python-11.8.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (16.5 MB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 16.5/16.5 MB 59.6 MB/s eta 0:00:00\nCollecting rmm-cu11==23.6.*\n  Downloading https://pypi.nvidia.com/rmm-cu11/rmm_cu11-23.6.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.7 MB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.7/1.7 MB 70.7 MB/s eta 0:00:00\nRequirement already satisfied: fsspec>=0.6.0 in /databricks/python3/lib/python3.10/site-packages (from cudf-cu11) (2022.7.1)\nRequirement already satisfied: cython in /databricks/python3/lib/python3.10/site-packages (from cuda-python<12.0,>=11.7.1->cudf-cu11) (0.29.32)\nCollecting fastrlock>=0.5\n  Downloading fastrlock-0.8.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_24_x86_64.whl (47 kB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 47.3/47.3 kB 10.5 MB/s eta 0:00:00\nCollecting llvmlite<0.41,>=0.40.0dev0\n  Downloading llvmlite-0.40.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (42.1 MB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 42.1/42.1 MB 36.7 MB/s eta 0:00:00\nRequirement already satisfied: python-dateutil>=2.8.1 in /databricks/python3/lib/python3.10/site-packages (from pandas<1.6.0dev0,>=1.3->cudf-cu11) (2.8.2)\nRequirement already satisfied: pytz>=2020.1 in /databricks/python3/lib/python3.10/site-packages (from pandas<1.6.0dev0,>=1.3->cudf-cu11) (2022.1)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /databricks/python3/lib/python3.10/site-packages (from packaging->cudf-cu11) (3.0.9)\nRequirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil>=2.8.1->pandas<1.6.0dev0,>=1.3->cudf-cu11) (1.16.0)\nInstalling collected packages: ptxcompiler-cu11, nvtx, fastrlock, cubinlinker-cu11, pyarrow, protobuf, llvmlite, cupy-cuda11x, cuda-python, numba, rmm-cu11, cudf-cu11\n  Attempting uninstall: pyarrow\n    Found existing installation: pyarrow 8.0.0\n    Not uninstalling pyarrow at /databricks/python3/lib/python3.10/site-packages, outside environment /local_disk0/.ephemeral_nfs/envs/pythonEnv-fa9614d8-7e9c-40b7-9e5e-09b1f38d56f7\n    Can't uninstall 'pyarrow'. No files were found to uninstall.\n  Attempting uninstall: protobuf\n    Found existing installation: protobuf 3.19.4\n    Not uninstalling protobuf at /databricks/python3/lib/python3.10/site-packages, outside environment /local_disk0/.ephemeral_nfs/envs/pythonEnv-fa9614d8-7e9c-40b7-9e5e-09b1f38d56f7\n    Can't uninstall 'protobuf'. No files were found to uninstall.\n  Attempting uninstall: llvmlite\n    Found existing installation: llvmlite 0.38.0\n    Not uninstalling llvmlite at /databricks/python3/lib/python3.10/site-packages, outside environment /local_disk0/.ephemeral_nfs/envs/pythonEnv-fa9614d8-7e9c-40b7-9e5e-09b1f38d56f7\n    Can't uninstall 'llvmlite'. No files were found to uninstall.\n  Attempting uninstall: numba\n    Found existing installation: numba 0.55.1\n    Not uninstalling numba at /databricks/python3/lib/python3.10/site-packages, outside environment /local_disk0/.ephemeral_nfs/envs/pythonEnv-fa9614d8-7e9c-40b7-9e5e-09b1f38d56f7\n    Can't uninstall 'numba'. No files were found to uninstall.\nERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\npetastorm 0.12.1 requires pyspark>=2.1.0, which is not installed.\ndatabricks-feature-store 0.14.0 requires pyspark<4,>=3.1.2, which is not installed.\ntensorflow 2.11.1 requires protobuf<3.20,>=3.9.2, but you have protobuf 4.21.12 which is incompatible.\ntensorboard 2.11.0 requires protobuf<4,>=3.9.2, but you have protobuf 4.21.12 which is incompatible.\nSuccessfully installed cubinlinker-cu11-0.3.0.post1 cuda-python-11.8.2 cudf-cu11-23.6.1 cupy-cuda11x-12.1.0 fastrlock-0.8.1 llvmlite-0.40.1 numba-0.57.1 nvtx-0.2.6 protobuf-4.21.12 ptxcompiler-cu11-0.7.0.post1 pyarrow-11.0.0 rmm-cu11-23.6.0\n\u001B[43mNote: you may need to restart the kernel using dbutils.library.restartPython() to use updated packages.\u001B[0m\n\u001B[43mNote: you may need to restart the kernel using dbutils.library.restartPython() to use updated packages.\u001B[0m\nLooking in indexes: https://pypi.org/simple, https://pypi.nvidia.com\nCollecting cuml-cu11\n  Downloading https://pypi.nvidia.com/cuml-cu11/cuml_cu11-23.6.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1079.0 MB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.1/1.1 GB 682.5 kB/s eta 0:00:00\nCollecting distributed==2023.3.2.1\n  Downloading distributed-2023.3.2.1-py3-none-any.whl (957 kB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 957.1/957.1 kB 9.2 MB/s eta 0:00:00\nCollecting dask==2023.3.2\n  Downloading dask-2023.3.2-py3-none-any.whl (1.2 MB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.2/1.2 MB 12.5 MB/s eta 0:00:00\nCollecting dask-cudf-cu11==23.6.*\n  Downloading https://pypi.nvidia.com/dask-cudf-cu11/dask_cudf_cu11-23.6.0-py3-none-any.whl (79 kB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 79.6/79.6 kB 16.2 MB/s eta 0:00:00\nRequirement already satisfied: cupy-cuda11x>=12.0.0 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-fa9614d8-7e9c-40b7-9e5e-09b1f38d56f7/lib/python3.10/site-packages (from cuml-cu11) (12.1.0)\nRequirement already satisfied: cudf-cu11==23.6.* in /local_disk0/.ephemeral_nfs/envs/pythonEnv-fa9614d8-7e9c-40b7-9e5e-09b1f38d56f7/lib/python3.10/site-packages (from cuml-cu11) (23.6.1)\nRequirement already satisfied: numba>=0.57 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-fa9614d8-7e9c-40b7-9e5e-09b1f38d56f7/lib/python3.10/site-packages (from cuml-cu11) (0.57.1)\nRequirement already satisfied: scipy in /databricks/python3/lib/python3.10/site-packages (from cuml-cu11) (1.9.1)\nRequirement already satisfied: joblib>=0.11 in /databricks/python3/lib/python3.10/site-packages (from cuml-cu11) (1.2.0)\nCollecting dask-cuda==23.6.*\n  Downloading dask_cuda-23.6.0-py3-none-any.whl (125 kB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 125.2/125.2 kB 16.2 MB/s eta 0:00:00\nCollecting raft-dask-cu11==23.6.*\n  Downloading https://pypi.nvidia.com/raft-dask-cu11/raft_dask_cu11-23.6.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (214.7 MB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 214.7/214.7 MB 6.6 MB/s eta 0:00:00\nCollecting treelite==3.2.0\n  Downloading treelite-3.2.0-py3-none-manylinux2014_x86_64.whl (1.0 MB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.0/1.0 MB 24.8 MB/s eta 0:00:00\nCollecting treelite-runtime==3.2.0\n  Downloading treelite_runtime-3.2.0-py3-none-manylinux2014_x86_64.whl (198 kB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 198.2/198.2 kB 21.2 MB/s eta 0:00:00\nRequirement already satisfied: protobuf<4.22,>=4.21.6 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-fa9614d8-7e9c-40b7-9e5e-09b1f38d56f7/lib/python3.10/site-packages (from cudf-cu11==23.6.*->cuml-cu11) (4.21.12)\nRequirement already satisfied: nvtx>=0.2.1 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-fa9614d8-7e9c-40b7-9e5e-09b1f38d56f7/lib/python3.10/site-packages (from cudf-cu11==23.6.*->cuml-cu11) (0.2.6)\nRequirement already satisfied: packaging in /databricks/python3/lib/python3.10/site-packages (from cudf-cu11==23.6.*->cuml-cu11) (21.3)\nRequirement already satisfied: cuda-python<12.0,>=11.7.1 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-fa9614d8-7e9c-40b7-9e5e-09b1f38d56f7/lib/python3.10/site-packages (from cudf-cu11==23.6.*->cuml-cu11) (11.8.2)\nRequirement already satisfied: fsspec>=0.6.0 in /databricks/python3/lib/python3.10/site-packages (from cudf-cu11==23.6.*->cuml-cu11) (2022.7.1)\nRequirement already satisfied: numpy>=1.21 in /databricks/python3/lib/python3.10/site-packages (from cudf-cu11==23.6.*->cuml-cu11) (1.21.5)\nRequirement already satisfied: pandas<1.6.0dev0,>=1.3 in /databricks/python3/lib/python3.10/site-packages (from cudf-cu11==23.6.*->cuml-cu11) (1.4.4)\nRequirement already satisfied: typing-extensions>=4.0.0 in /databricks/python3/lib/python3.10/site-packages (from cudf-cu11==23.6.*->cuml-cu11) (4.3.0)\nRequirement already satisfied: cubinlinker-cu11 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-fa9614d8-7e9c-40b7-9e5e-09b1f38d56f7/lib/python3.10/site-packages (from cudf-cu11==23.6.*->cuml-cu11) (0.3.0.post1)\nRequirement already satisfied: rmm-cu11==23.6.* in /local_disk0/.ephemeral_nfs/envs/pythonEnv-fa9614d8-7e9c-40b7-9e5e-09b1f38d56f7/lib/python3.10/site-packages (from cudf-cu11==23.6.*->cuml-cu11) (23.6.0)\nRequirement already satisfied: cachetools in /databricks/python3/lib/python3.10/site-packages (from cudf-cu11==23.6.*->cuml-cu11) (4.2.4)\nRequirement already satisfied: ptxcompiler-cu11 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-fa9614d8-7e9c-40b7-9e5e-09b1f38d56f7/lib/python3.10/site-packages (from cudf-cu11==23.6.*->cuml-cu11) (0.7.0.post1)\nRequirement already satisfied: pyarrow==11.* in /local_disk0/.ephemeral_nfs/envs/pythonEnv-fa9614d8-7e9c-40b7-9e5e-09b1f38d56f7/lib/python3.10/site-packages (from cudf-cu11==23.6.*->cuml-cu11) (11.0.0)\nRequirement already satisfied: click>=7.0 in /databricks/python3/lib/python3.10/site-packages (from dask==2023.3.2->cuml-cu11) (8.0.4)\nCollecting toolz>=0.8.2\n  Downloading toolz-0.12.0-py3-none-any.whl (55 kB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 55.8/55.8 kB 8.3 MB/s eta 0:00:00\nCollecting importlib-metadata>=4.13.0\n  Downloading importlib_metadata-6.8.0-py3-none-any.whl (22 kB)\nRequirement already satisfied: cloudpickle>=1.1.1 in /databricks/python3/lib/python3.10/site-packages (from dask==2023.3.2->cuml-cu11) (2.0.0)\nCollecting partd>=1.2.0\n  Downloading partd-1.4.0-py3-none-any.whl (18 kB)\nRequirement already satisfied: pyyaml>=5.3.1 in /databricks/python3/lib/python3.10/site-packages (from dask==2023.3.2->cuml-cu11) (6.0)\nCollecting pynvml<11.5,>=11.0.0\n  Downloading pynvml-11.4.1-py3-none-any.whl (46 kB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 47.0/47.0 kB 8.3 MB/s eta 0:00:00\nCollecting zict>=2.0.0\n  Downloading zict-3.0.0-py2.py3-none-any.whl (43 kB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 43.3/43.3 kB 9.5 MB/s eta 0:00:00\nCollecting tblib>=1.6.0\n  Downloading tblib-2.0.0-py3-none-any.whl (11 kB)\nRequirement already satisfied: msgpack>=1.0.0 in /databricks/python3/lib/python3.10/site-packages (from distributed==2023.3.2.1->cuml-cu11) (1.0.5)\nCollecting sortedcontainers>=2.0.5\n  Downloading sortedcontainers-2.4.0-py2.py3-none-any.whl (29 kB)\nRequirement already satisfied: tornado>=6.0.3 in /databricks/python3/lib/python3.10/site-packages (from distributed==2023.3.2.1->cuml-cu11) (6.1)\nCollecting locket>=1.0.0\n  Downloading locket-1.0.0-py2.py3-none-any.whl (4.4 kB)\nRequirement already satisfied: urllib3>=1.24.3 in /databricks/python3/lib/python3.10/site-packages (from distributed==2023.3.2.1->cuml-cu11) (1.26.11)\nRequirement already satisfied: jinja2>=2.10.3 in /databricks/python3/lib/python3.10/site-packages (from distributed==2023.3.2.1->cuml-cu11) (2.11.3)\nRequirement already satisfied: psutil>=5.7.0 in /databricks/python3/lib/python3.10/site-packages (from distributed==2023.3.2.1->cuml-cu11) (5.9.0)\nCollecting ucx-py-cu11==0.32.*\n  Downloading https://pypi.nvidia.com/ucx-py-cu11/ucx_py_cu11-0.32.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.9 MB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 7.9/7.9 MB 70.5 MB/s eta 0:00:00\nCollecting pylibraft-cu11==23.6.*\n  Downloading https://pypi.nvidia.com/pylibraft-cu11/pylibraft_cu11-23.6.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (471.7 MB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 471.7/471.7 MB 1.5 MB/s eta 0:00:00\nRequirement already satisfied: fastrlock>=0.5 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-fa9614d8-7e9c-40b7-9e5e-09b1f38d56f7/lib/python3.10/site-packages (from cupy-cuda11x>=12.0.0->cuml-cu11) (0.8.1)\nRequirement already satisfied: llvmlite<0.41,>=0.40.0dev0 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-fa9614d8-7e9c-40b7-9e5e-09b1f38d56f7/lib/python3.10/site-packages (from numba>=0.57->cuml-cu11) (0.40.1)\nRequirement already satisfied: cython in /databricks/python3/lib/python3.10/site-packages (from cuda-python<12.0,>=11.7.1->cudf-cu11==23.6.*->cuml-cu11) (0.29.32)\nRequirement already satisfied: zipp>=0.5 in /databricks/python3/lib/python3.10/site-packages (from importlib-metadata>=4.13.0->dask==2023.3.2->cuml-cu11) (3.8.0)\nRequirement already satisfied: MarkupSafe>=0.23 in /databricks/python3/lib/python3.10/site-packages (from jinja2>=2.10.3->distributed==2023.3.2.1->cuml-cu11) (2.0.1)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /databricks/python3/lib/python3.10/site-packages (from packaging->cudf-cu11==23.6.*->cuml-cu11) (3.0.9)\nRequirement already satisfied: python-dateutil>=2.8.1 in /databricks/python3/lib/python3.10/site-packages (from pandas<1.6.0dev0,>=1.3->cudf-cu11==23.6.*->cuml-cu11) (2.8.2)\nRequirement already satisfied: pytz>=2020.1 in /databricks/python3/lib/python3.10/site-packages (from pandas<1.6.0dev0,>=1.3->cudf-cu11==23.6.*->cuml-cu11) (2022.1)\nRequirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil>=2.8.1->pandas<1.6.0dev0,>=1.3->cudf-cu11==23.6.*->cuml-cu11) (1.16.0)\nInstalling collected packages: sortedcontainers, zict, toolz, tblib, pynvml, locket, importlib-metadata, ucx-py-cu11, treelite-runtime, treelite, partd, pylibraft-cu11, dask, distributed, dask-cudf-cu11, dask-cuda, raft-dask-cu11, cuml-cu11\n  Attempting uninstall: importlib-metadata\n    Found existing installation: importlib-metadata 4.11.3\n    Not uninstalling importlib-metadata at /databricks/python3/lib/python3.10/site-packages, outside environment /local_disk0/.ephemeral_nfs/envs/pythonEnv-fa9614d8-7e9c-40b7-9e5e-09b1f38d56f7\n    Can't uninstall 'importlib-metadata'. No files were found to uninstall.\nERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ndatabricks-feature-store 0.14.0 requires pyspark<4,>=3.1.2, which is not installed.\nSuccessfully installed cuml-cu11-23.6.0 dask-2023.3.2 dask-cuda-23.6.0 dask-cudf-cu11-23.6.0 distributed-2023.3.2.1 importlib-metadata-6.8.0 locket-1.0.0 partd-1.4.0 pylibraft-cu11-23.6.2 pynvml-11.4.1 raft-dask-cu11-23.6.2 sortedcontainers-2.4.0 tblib-2.0.0 toolz-0.12.0 treelite-3.2.0 treelite-runtime-3.2.0 ucx-py-cu11-0.32.0 zict-3.0.0\n\u001B[43mNote: you may need to restart the kernel using dbutils.library.restartPython() to use updated packages.\u001B[0m\n"
     ]
    }
   ],
   "source": [
    "%pip install --extra-index-url https://pypi.nvidia.com cudf-cu11\n",
    "%pip install cuml-cu11 --extra-index-url https://pypi.nvidia.com"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4db6d74e-0cb7-4ff4-adfb-bd3f71290df1",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[43mNote: you may need to restart the kernel using dbutils.library.restartPython() to use updated packages.\u001B[0m\nCollecting openpyxl\n  Downloading openpyxl-3.1.2-py2.py3-none-any.whl (249 kB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 250.0/250.0 kB 5.8 MB/s eta 0:00:00\nCollecting et-xmlfile\n  Downloading et_xmlfile-1.1.0-py3-none-any.whl (4.7 kB)\nInstalling collected packages: et-xmlfile, openpyxl\nSuccessfully installed et-xmlfile-1.1.0 openpyxl-3.1.2\n\u001B[43mNote: you may need to restart the kernel using dbutils.library.restartPython() to use updated packages.\u001B[0m\n\u001B[43mNote: you may need to restart the kernel using dbutils.library.restartPython() to use updated packages.\u001B[0m\nCollecting scikit-image\n  Downloading scikit_image-0.21.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.8 MB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 13.8/13.8 MB 50.8 MB/s eta 0:00:00\nCollecting imageio>=2.27\n  Downloading imageio-2.31.1-py3-none-any.whl (313 kB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 313.2/313.2 kB 36.8 MB/s eta 0:00:00\nCollecting tifffile>=2022.8.12\n  Downloading tifffile-2023.7.18-py3-none-any.whl (221 kB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 221.4/221.4 kB 36.8 MB/s eta 0:00:00\nRequirement already satisfied: lazy_loader>=0.2 in /databricks/python3/lib/python3.10/site-packages (from scikit-image) (0.3)\nRequirement already satisfied: pillow>=9.0.1 in /databricks/python3/lib/python3.10/site-packages (from scikit-image) (9.2.0)\nRequirement already satisfied: packaging>=21 in /databricks/python3/lib/python3.10/site-packages (from scikit-image) (21.3)\nRequirement already satisfied: scipy>=1.8 in /databricks/python3/lib/python3.10/site-packages (from scikit-image) (1.9.1)\nRequirement already satisfied: networkx>=2.8 in /databricks/python3/lib/python3.10/site-packages (from scikit-image) (2.8.4)\nRequirement already satisfied: numpy>=1.21.1 in /databricks/python3/lib/python3.10/site-packages (from scikit-image) (1.21.5)\nRequirement already satisfied: PyWavelets>=1.1.1 in /databricks/python3/lib/python3.10/site-packages (from scikit-image) (1.3.0)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /databricks/python3/lib/python3.10/site-packages (from packaging>=21->scikit-image) (3.0.9)\nInstalling collected packages: tifffile, imageio, scikit-image\nSuccessfully installed imageio-2.31.1 scikit-image-0.21.0 tifffile-2023.7.18\n\u001B[43mNote: you may need to restart the kernel using dbutils.library.restartPython() to use updated packages.\u001B[0m\n\u001B[43mNote: you may need to restart the kernel using dbutils.library.restartPython() to use updated packages.\u001B[0m\nCollecting dask-ml\n  Downloading dask_ml-2023.3.24-py3-none-any.whl (148 kB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 148.7/148.7 kB 4.2 MB/s eta 0:00:00\nRequirement already satisfied: pandas>=0.24.2 in /databricks/python3/lib/python3.10/site-packages (from dask-ml) (1.4.4)\nCollecting dask-glm>=0.2.0\n  Downloading dask_glm-0.2.0-py2.py3-none-any.whl (12 kB)\nRequirement already satisfied: scipy in /databricks/python3/lib/python3.10/site-packages (from dask-ml) (1.9.1)\nRequirement already satisfied: packaging in /databricks/python3/lib/python3.10/site-packages (from dask-ml) (21.3)\nRequirement already satisfied: numba>=0.51.0 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-fa9614d8-7e9c-40b7-9e5e-09b1f38d56f7/lib/python3.10/site-packages (from dask-ml) (0.57.1)\nRequirement already satisfied: distributed>=2.4.0 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-fa9614d8-7e9c-40b7-9e5e-09b1f38d56f7/lib/python3.10/site-packages (from dask-ml) (2023.3.2.1)\nCollecting multipledispatch>=0.4.9\n  Downloading multipledispatch-1.0.0-py3-none-any.whl (12 kB)\nCollecting scikit-learn>=1.2.0\n  Downloading scikit_learn-1.3.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (10.8 MB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 10.8/10.8 MB 36.0 MB/s eta 0:00:00\nRequirement already satisfied: dask[array,dataframe]>=2.4.0 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-fa9614d8-7e9c-40b7-9e5e-09b1f38d56f7/lib/python3.10/site-packages (from dask-ml) (2023.3.2)\nRequirement already satisfied: numpy>=1.20.0 in /databricks/python3/lib/python3.10/site-packages (from dask-ml) (1.21.5)\nRequirement already satisfied: cloudpickle>=0.2.2 in /databricks/python3/lib/python3.10/site-packages (from dask-glm>=0.2.0->dask-ml) (2.0.0)\nRequirement already satisfied: click>=7.0 in /databricks/python3/lib/python3.10/site-packages (from dask[array,dataframe]>=2.4.0->dask-ml) (8.0.4)\nRequirement already satisfied: pyyaml>=5.3.1 in /databricks/python3/lib/python3.10/site-packages (from dask[array,dataframe]>=2.4.0->dask-ml) (6.0)\nRequirement already satisfied: importlib-metadata>=4.13.0 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-fa9614d8-7e9c-40b7-9e5e-09b1f38d56f7/lib/python3.10/site-packages (from dask[array,dataframe]>=2.4.0->dask-ml) (6.8.0)\nRequirement already satisfied: toolz>=0.8.2 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-fa9614d8-7e9c-40b7-9e5e-09b1f38d56f7/lib/python3.10/site-packages (from dask[array,dataframe]>=2.4.0->dask-ml) (0.12.0)\nRequirement already satisfied: partd>=1.2.0 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-fa9614d8-7e9c-40b7-9e5e-09b1f38d56f7/lib/python3.10/site-packages (from dask[array,dataframe]>=2.4.0->dask-ml) (1.4.0)\nRequirement already satisfied: fsspec>=0.6.0 in /databricks/python3/lib/python3.10/site-packages (from dask[array,dataframe]>=2.4.0->dask-ml) (2022.7.1)\nRequirement already satisfied: tornado>=6.0.3 in /databricks/python3/lib/python3.10/site-packages (from distributed>=2.4.0->dask-ml) (6.1)\nRequirement already satisfied: urllib3>=1.24.3 in /databricks/python3/lib/python3.10/site-packages (from distributed>=2.4.0->dask-ml) (1.26.11)\nRequirement already satisfied: jinja2>=2.10.3 in /databricks/python3/lib/python3.10/site-packages (from distributed>=2.4.0->dask-ml) (2.11.3)\nRequirement already satisfied: zict>=2.1.0 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-fa9614d8-7e9c-40b7-9e5e-09b1f38d56f7/lib/python3.10/site-packages (from distributed>=2.4.0->dask-ml) (3.0.0)\nRequirement already satisfied: locket>=1.0.0 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-fa9614d8-7e9c-40b7-9e5e-09b1f38d56f7/lib/python3.10/site-packages (from distributed>=2.4.0->dask-ml) (1.0.0)\nRequirement already satisfied: sortedcontainers>=2.0.5 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-fa9614d8-7e9c-40b7-9e5e-09b1f38d56f7/lib/python3.10/site-packages (from distributed>=2.4.0->dask-ml) (2.4.0)\nRequirement already satisfied: tblib>=1.6.0 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-fa9614d8-7e9c-40b7-9e5e-09b1f38d56f7/lib/python3.10/site-packages (from distributed>=2.4.0->dask-ml) (2.0.0)\nRequirement already satisfied: msgpack>=1.0.0 in /databricks/python3/lib/python3.10/site-packages (from distributed>=2.4.0->dask-ml) (1.0.5)\nRequirement already satisfied: psutil>=5.7.0 in /databricks/python3/lib/python3.10/site-packages (from distributed>=2.4.0->dask-ml) (5.9.0)\nRequirement already satisfied: llvmlite<0.41,>=0.40.0dev0 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-fa9614d8-7e9c-40b7-9e5e-09b1f38d56f7/lib/python3.10/site-packages (from numba>=0.51.0->dask-ml) (0.40.1)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /databricks/python3/lib/python3.10/site-packages (from packaging->dask-ml) (3.0.9)\nRequirement already satisfied: python-dateutil>=2.8.1 in /databricks/python3/lib/python3.10/site-packages (from pandas>=0.24.2->dask-ml) (2.8.2)\nRequirement already satisfied: pytz>=2020.1 in /databricks/python3/lib/python3.10/site-packages (from pandas>=0.24.2->dask-ml) (2022.1)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /databricks/python3/lib/python3.10/site-packages (from scikit-learn>=1.2.0->dask-ml) (2.2.0)\nRequirement already satisfied: joblib>=1.1.1 in /databricks/python3/lib/python3.10/site-packages (from scikit-learn>=1.2.0->dask-ml) (1.2.0)\nRequirement already satisfied: zipp>=0.5 in /databricks/python3/lib/python3.10/site-packages (from importlib-metadata>=4.13.0->dask[array,dataframe]>=2.4.0->dask-ml) (3.8.0)\nRequirement already satisfied: MarkupSafe>=0.23 in /databricks/python3/lib/python3.10/site-packages (from jinja2>=2.10.3->distributed>=2.4.0->dask-ml) (2.0.1)\nRequirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil>=2.8.1->pandas>=0.24.2->dask-ml) (1.16.0)\nInstalling collected packages: multipledispatch, scikit-learn, dask-glm, dask-ml\n  Attempting uninstall: scikit-learn\n    Found existing installation: scikit-learn 1.1.1\n    Not uninstalling scikit-learn at /databricks/python3/lib/python3.10/site-packages, outside environment /local_disk0/.ephemeral_nfs/envs/pythonEnv-fa9614d8-7e9c-40b7-9e5e-09b1f38d56f7\n    Can't uninstall 'scikit-learn'. No files were found to uninstall.\nERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nmleap 0.20.0 requires scikit-learn<0.23.0,>=0.22.0, but you have scikit-learn 1.3.0 which is incompatible.\nSuccessfully installed dask-glm-0.2.0 dask-ml-2023.3.24 multipledispatch-1.0.0 scikit-learn-1.3.0\n\u001B[43mNote: you may need to restart the kernel using dbutils.library.restartPython() to use updated packages.\u001B[0m\n"
     ]
    }
   ],
   "source": [
    "%pip install openpyxl\n",
    "%pip install scikit-image\n",
    "%pip install dask-ml\n",
    "dbutils.library.restartPython()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fb515429-ce68-491f-8c62-20e915bf0752",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "xgboost version 1.7.6\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/local_disk0/.ephemeral_nfs/envs/pythonEnv-fa9614d8-7e9c-40b7-9e5e-09b1f38d56f7/lib/python3.10/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.\nPerhaps you already have a cluster running?\nHosting the HTTP server on port 37865 instead\n  warnings.warn(\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/html": [
       "<div>\n",
       "    <div style=\"width: 24px; height: 24px; background-color: #e1e1e1; border: 3px solid #9D9D9D; border-radius: 5px; position: absolute;\"> </div>\n",
       "    <div style=\"margin-left: 48px;\">\n",
       "        <h3 style=\"margin-bottom: 0px;\">Client</h3>\n",
       "        <p style=\"color: #9D9D9D; margin-bottom: 0px;\">Client-40901665-32e2-11ee-8d8b-00163e889c3c</p>\n",
       "        <table style=\"width: 100%; text-align: left;\">\n",
       "\n",
       "        <tr>\n",
       "        \n",
       "            <td style=\"text-align: left;\"><strong>Connection method:</strong> Cluster object</td>\n",
       "            <td style=\"text-align: left;\"><strong>Cluster type:</strong> distributed.LocalCluster</td>\n",
       "        \n",
       "        </tr>\n",
       "\n",
       "        \n",
       "            <tr>\n",
       "                <td style=\"text-align: left;\">\n",
       "                    <strong>Dashboard: </strong> <a href=\"http://127.0.0.1:37865/status\" target=\"_blank\">http://127.0.0.1:37865/status</a>\n",
       "                </td>\n",
       "                <td style=\"text-align: left;\"></td>\n",
       "            </tr>\n",
       "        \n",
       "\n",
       "        </table>\n",
       "\n",
       "        \n",
       "\n",
       "        \n",
       "            <details>\n",
       "            <summary style=\"margin-bottom: 20px;\"><h3 style=\"display: inline;\">Cluster Info</h3></summary>\n",
       "            <div class=\"jp-RenderedHTMLCommon jp-RenderedHTML jp-mod-trusted jp-OutputArea-output\">\n",
       "    <div style=\"width: 24px; height: 24px; background-color: #e1e1e1; border: 3px solid #9D9D9D; border-radius: 5px; position: absolute;\">\n",
       "    </div>\n",
       "    <div style=\"margin-left: 48px;\">\n",
       "        <h3 style=\"margin-bottom: 0px; margin-top: 0px;\">LocalCluster</h3>\n",
       "        <p style=\"color: #9D9D9D; margin-bottom: 0px;\">d529a490</p>\n",
       "        <table style=\"width: 100%; text-align: left;\">\n",
       "            <tr>\n",
       "                <td style=\"text-align: left;\">\n",
       "                    <strong>Dashboard:</strong> <a href=\"http://127.0.0.1:37865/status\" target=\"_blank\">http://127.0.0.1:37865/status</a>\n",
       "                </td>\n",
       "                <td style=\"text-align: left;\">\n",
       "                    <strong>Workers:</strong> 1\n",
       "                </td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                <td style=\"text-align: left;\">\n",
       "                    <strong>Total threads:</strong> 32\n",
       "                </td>\n",
       "                <td style=\"text-align: left;\">\n",
       "                    <strong>Total memory:</strong> 10.83 GiB\n",
       "                </td>\n",
       "            </tr>\n",
       "            \n",
       "            <tr>\n",
       "    <td style=\"text-align: left;\"><strong>Status:</strong> running</td>\n",
       "    <td style=\"text-align: left;\"><strong>Using processes:</strong> True</td>\n",
       "</tr>\n",
       "\n",
       "            \n",
       "        </table>\n",
       "\n",
       "        <details>\n",
       "            <summary style=\"margin-bottom: 20px;\">\n",
       "                <h3 style=\"display: inline;\">Scheduler Info</h3>\n",
       "            </summary>\n",
       "\n",
       "            <div style=\"\">\n",
       "    <div>\n",
       "        <div style=\"width: 24px; height: 24px; background-color: #FFF7E5; border: 3px solid #FF6132; border-radius: 5px; position: absolute;\"> </div>\n",
       "        <div style=\"margin-left: 48px;\">\n",
       "            <h3 style=\"margin-bottom: 0px;\">Scheduler</h3>\n",
       "            <p style=\"color: #9D9D9D; margin-bottom: 0px;\">Scheduler-7c78a2eb-b293-4aba-ba08-5dae33d495c9</p>\n",
       "            <table style=\"width: 100%; text-align: left;\">\n",
       "                <tr>\n",
       "                    <td style=\"text-align: left;\">\n",
       "                        <strong>Comm:</strong> tcp://127.0.0.1:44681\n",
       "                    </td>\n",
       "                    <td style=\"text-align: left;\">\n",
       "                        <strong>Workers:</strong> 1\n",
       "                    </td>\n",
       "                </tr>\n",
       "                <tr>\n",
       "                    <td style=\"text-align: left;\">\n",
       "                        <strong>Dashboard:</strong> <a href=\"http://127.0.0.1:37865/status\" target=\"_blank\">http://127.0.0.1:37865/status</a>\n",
       "                    </td>\n",
       "                    <td style=\"text-align: left;\">\n",
       "                        <strong>Total threads:</strong> 32\n",
       "                    </td>\n",
       "                </tr>\n",
       "                <tr>\n",
       "                    <td style=\"text-align: left;\">\n",
       "                        <strong>Started:</strong> Just now\n",
       "                    </td>\n",
       "                    <td style=\"text-align: left;\">\n",
       "                        <strong>Total memory:</strong> 10.83 GiB\n",
       "                    </td>\n",
       "                </tr>\n",
       "            </table>\n",
       "        </div>\n",
       "    </div>\n",
       "\n",
       "    <details style=\"margin-left: 48px;\">\n",
       "        <summary style=\"margin-bottom: 20px;\">\n",
       "            <h3 style=\"display: inline;\">Workers</h3>\n",
       "        </summary>\n",
       "\n",
       "        \n",
       "        <div style=\"margin-bottom: 20px;\">\n",
       "            <div style=\"width: 24px; height: 24px; background-color: #DBF5FF; border: 3px solid #4CC9FF; border-radius: 5px; position: absolute;\"> </div>\n",
       "            <div style=\"margin-left: 48px;\">\n",
       "            <details>\n",
       "                <summary>\n",
       "                    <h4 style=\"margin-bottom: 0px; display: inline;\">Worker: 0</h4>\n",
       "                </summary>\n",
       "                <table style=\"width: 100%; text-align: left;\">\n",
       "                    <tr>\n",
       "                        <td style=\"text-align: left;\">\n",
       "                            <strong>Comm: </strong> tcp://127.0.0.1:45443\n",
       "                        </td>\n",
       "                        <td style=\"text-align: left;\">\n",
       "                            <strong>Total threads: </strong> 32\n",
       "                        </td>\n",
       "                    </tr>\n",
       "                    <tr>\n",
       "                        <td style=\"text-align: left;\">\n",
       "                            <strong>Dashboard: </strong> <a href=\"http://127.0.0.1:40381/status\" target=\"_blank\">http://127.0.0.1:40381/status</a>\n",
       "                        </td>\n",
       "                        <td style=\"text-align: left;\">\n",
       "                            <strong>Memory: </strong> 10.83 GiB\n",
       "                        </td>\n",
       "                    </tr>\n",
       "                    <tr>\n",
       "                        <td style=\"text-align: left;\">\n",
       "                            <strong>Nanny: </strong> tcp://127.0.0.1:42759\n",
       "                        </td>\n",
       "                        <td style=\"text-align: left;\"></td>\n",
       "                    </tr>\n",
       "                    <tr>\n",
       "                        <td colspan=\"2\" style=\"text-align: left;\">\n",
       "                            <strong>Local directory: </strong> /tmp/dask-worker-space/worker-1d87__yx\n",
       "                        </td>\n",
       "                    </tr>\n",
       "\n",
       "                    \n",
       "                    <tr>\n",
       "                        <td style=\"text-align: left;\">\n",
       "                            <strong>GPU: </strong>Tesla T4\n",
       "                        </td>\n",
       "                        <td style=\"text-align: left;\">\n",
       "                            <strong>GPU memory: </strong> 14.76 GiB\n",
       "                        </td>\n",
       "                    </tr>\n",
       "                    \n",
       "\n",
       "                    \n",
       "\n",
       "                </table>\n",
       "            </details>\n",
       "            </div>\n",
       "        </div>\n",
       "        \n",
       "\n",
       "    </details>\n",
       "</div>\n",
       "\n",
       "        </details>\n",
       "    </div>\n",
       "</div>\n",
       "            </details>\n",
       "        \n",
       "\n",
       "    </div>\n",
       "</div>"
      ],
      "text/plain": [
       "<Client: 'tcp://127.0.0.1:44681' processes=1 threads=32, memory=10.83 GiB>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import (\n",
    "    RandomizedSearchCV,\n",
    "    train_test_split,\n",
    "    StratifiedKFold,\n",
    ")\n",
    "from sklearn.pipeline import Pipeline\n",
    "#from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from cuml.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import (\n",
    "    confusion_matrix,\n",
    "    ConfusionMatrixDisplay,\n",
    "    precision_recall_fscore_support,\n",
    "    f1_score,\n",
    "    recall_score,\n",
    "    precision_score,\n",
    ")\n",
    "from xgboost import XGBClassifier\n",
    "import xgboost as xgb\n",
    "import scipy.stats as st\n",
    "import mlflow\n",
    "from mlflow.tracking import MlflowClient\n",
    "import matplotlib.pyplot as plt\n",
    "from mlflow.utils.environment import _mlflow_conda_env\n",
    "from mlflow.models.signature import infer_signature\n",
    "import matplotlib\n",
    "import io\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import sklearn\n",
    "import cuml\n",
    "import cudf\n",
    "\n",
    "import dask\n",
    "import dask.dataframe as dd\n",
    "from dask.distributed import Client, wait, progress, get_worker\n",
    "from xgboost.dask import DaskDMatrix\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.utils import compute_sample_weight\n",
    "\n",
    "\n",
    "print('xgboost version', xgb.__version__)\n",
    "\n",
    "run_type = 'cpu'\n",
    "max_depth = 15\n",
    "num_boost_round = 200\n",
    "score_metric = 'auc'\n",
    "storage_backend = 'local'\n",
    "\n",
    "DATA_PATH = '/dbfs/FileStore/tables/processed_data_june_dask.parquet'\n",
    "included_features = ['TAGS', 'ACTION', 'OBJECT', 'PRIOR_ACTIONS', 'ROOT CAUSE', 'screenshot', 'IssueType', 'CAT']\n",
    "label_col = 'CAT'\n",
    "xgb_model_name = 'xgb_'+run_type+'_autoresult_analysis_max_depth_'+str(max_depth)\n",
    "\n",
    "if run_type == 'gpu':\n",
    "    from dask_cuda import LocalCUDACluster\n",
    "    \n",
    "    # Run on all available GPU on same machine:\n",
    "    cluster = LocalCUDACluster(threads_per_worker=16, memory_limit='128GB')\n",
    "    client = Client(cluster)\n",
    "    \n",
    "    # Run single GPU:\n",
    "#     client = Client(n_workers=1, threads_per_worker=32)\n",
    "elif run_type == 'cpu':\n",
    "    client = Client(n_workers=1, threads_per_worker=32)\n",
    "#     client = Client(n_workers=2, threads_per_worker=16)   \n",
    "    \n",
    "client"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ab418fc2-c4e1-4406-a624-c6d620d2759c",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0a27aecc-79f6-40c1-93a6-e25f6f5bba8b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Already mounted!\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>path</th><th>name</th><th>size</th><th>modificationTime</th></tr></thead><tbody><tr><td>dbfs:/mnt/s3_mount/nvirginia-prod/</td><td>nvirginia-prod/</td><td>0</td><td>1691165785481</td></tr><tr><td>dbfs:/mnt/s3_mount/qe_automation/</td><td>qe_automation/</td><td>0</td><td>1691165785481</td></tr><tr><td>dbfs:/mnt/s3_mount/weights/</td><td>weights/</td><td>0</td><td>1691165785481</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "dbfs:/mnt/s3_mount/nvirginia-prod/",
         "nvirginia-prod/",
         0,
         1691165785481
        ],
        [
         "dbfs:/mnt/s3_mount/qe_automation/",
         "qe_automation/",
         0,
         1691165785481
        ],
        [
         "dbfs:/mnt/s3_mount/weights/",
         "weights/",
         0,
         1691165785481
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "path",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "name",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "size",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "modificationTime",
         "type": "\"long\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "PATH = \"/dbfs/mnt/s3_mount/qe_automation/data/\"\n",
    "aws_bucket_name = \"ml-hub-workspace-root-sg50\"\n",
    "mount_name = \"s3_mount\"\n",
    "mount_point = \"/mnt/%s\" % mount_name\n",
    "try:\n",
    "    dbutils.fs.mount(\"s3a://%s\" % aws_bucket_name, mount_point)\n",
    "except Exception as e:\n",
    "    print(\"Already mounted!\")\n",
    "#dbutils.fs.mount(f\"s3a://{aws_bucket_name}\", f\"/mnt/{mount_name}\")\n",
    "display(dbutils.fs.ls(f\"/mnt/{mount_name}\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d9f01a64-65fe-49a1-9863-49517aa694cb",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[0m\u001B[34;42mnvirginia-prod\u001B[0m/  \u001B[34;42mqe_automation\u001B[0m/  \u001B[34;42mweights\u001B[0m/\r\n"
     ]
    }
   ],
   "source": [
    "ls /dbfs/mnt/s3_mount/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4b18a403-581f-4b04-ae0d-e9d87302f797",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/html": [
       "<div><strong>Dask DataFrame Structure:</strong></div>\n",
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ExecutionItemID</th>\n",
       "      <th>StepLogID</th>\n",
       "      <th>IssueType</th>\n",
       "      <th>TestCaseStatus</th>\n",
       "      <th>StepDescription</th>\n",
       "      <th>screenshot</th>\n",
       "      <th>ExecutionId</th>\n",
       "      <th>image_id</th>\n",
       "      <th>CAT</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>npartitions=1</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <td>int64</td>\n",
       "      <td>int64</td>\n",
       "      <td>object</td>\n",
       "      <td>object</td>\n",
       "      <td>object</td>\n",
       "      <td>object</td>\n",
       "      <td>int64</td>\n",
       "      <td>object</td>\n",
       "      <td>int64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>\n",
       "<div>Dask Name: assign, 4 graph layers</div>"
      ],
      "text/plain": [
       "Dask DataFrame Structure:\n",
       "              ExecutionItemID StepLogID IssueType TestCaseStatus StepDescription screenshot ExecutionId image_id    CAT\n",
       "npartitions=1                                                                                                          \n",
       "                        int64     int64    object         object          object     object       int64   object  int64\n",
       "                          ...       ...       ...            ...             ...        ...         ...      ...    ...\n",
       "Dask Name: assign, 4 graph layers"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "encode_dict = {}\n",
    "def encode_label(x):\n",
    "    if x not in encode_dict.keys():\n",
    "        encode_dict[x] = len(encode_dict)\n",
    "    return encode_dict[x]\n",
    "\n",
    "#df = pd.read_csv('/dbfs/FileStore/shared_uploads/Chris.Jose@cvent.com/subset.csv')\n",
    "df = dd.read_parquet('/dbfs/FileStore/tables/data_optimized_proc.parquet')\n",
    "df[\"CAT\"] = df[\"IssueType\"].map(lambda x: encode_label(x))\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1678adf0-7f56-4c2d-ba54-95f5c8b55a73",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n",
       "\u001B[0;31mAttributeError\u001B[0m                            Traceback (most recent call last)\n",
       "File \u001B[0;32m<command-4152036570402745>, line 1\u001B[0m\n",
       "\u001B[0;32m----> 1\u001B[0m \u001B[43mdd\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43miloc\u001B[49m[\u001B[38;5;241m22\u001B[39m]\n",
       "\n",
       "\u001B[0;31mAttributeError\u001B[0m: module 'dask.dataframe' has no attribute 'iloc'"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n\u001B[0;31mAttributeError\u001B[0m                            Traceback (most recent call last)\nFile \u001B[0;32m<command-4152036570402745>, line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m \u001B[43mdd\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43miloc\u001B[49m[\u001B[38;5;241m22\u001B[39m]\n\n\u001B[0;31mAttributeError\u001B[0m: module 'dask.dataframe' has no attribute 'iloc'",
       "errorSummary": "<span class='ansi-red-fg'>AttributeError</span>: module 'dask.dataframe' has no attribute 'iloc'",
       "errorTraceType": "ansi",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "dd.iloc[22]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f655bedf-570a-4e06-8685-b7a9b9f94322",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PARSED:  17800\nSUBSET:  Delayed('int-6820ab00-cf34-4be1-98b1-efbb0b398d63')\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n",
       "\u001B[0;31mTypeError\u001B[0m                                 Traceback (most recent call last)\n",
       "File \u001B[0;32m<command-2862209793706248>, line 53\u001B[0m\n",
       "\u001B[1;32m     51\u001B[0m parsed_ds \u001B[38;5;241m=\u001B[39m parsed_ds\u001B[38;5;241m.\u001B[39mreset_index(drop\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m)\n",
       "\u001B[1;32m     52\u001B[0m subset \u001B[38;5;241m=\u001B[39m subset\u001B[38;5;241m.\u001B[39mreset_index(drop\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m)\n",
       "\u001B[0;32m---> 53\u001B[0m final_df \u001B[38;5;241m=\u001B[39m pd\u001B[38;5;241m.\u001B[39mconcat([subset, parsed_ds], axis\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1\u001B[39m)\n",
       "\u001B[1;32m     54\u001B[0m \u001B[38;5;66;03m#final_df.reset_index(inplace=True)\u001B[39;00m\n",
       "\u001B[1;32m     55\u001B[0m final_df\u001B[38;5;241m.\u001B[39mto_parquet(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m/dbfs/FileStore/tables/processed_data_june_dask.parquet\u001B[39m\u001B[38;5;124m'\u001B[39m)\n",
       "\n",
       "File \u001B[0;32m/databricks/python/lib/python3.10/site-packages/pandas/util/_decorators.py:311\u001B[0m, in \u001B[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n",
       "\u001B[1;32m    305\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(args) \u001B[38;5;241m>\u001B[39m num_allow_args:\n",
       "\u001B[1;32m    306\u001B[0m     warnings\u001B[38;5;241m.\u001B[39mwarn(\n",
       "\u001B[1;32m    307\u001B[0m         msg\u001B[38;5;241m.\u001B[39mformat(arguments\u001B[38;5;241m=\u001B[39marguments),\n",
       "\u001B[1;32m    308\u001B[0m         \u001B[38;5;167;01mFutureWarning\u001B[39;00m,\n",
       "\u001B[1;32m    309\u001B[0m         stacklevel\u001B[38;5;241m=\u001B[39mstacklevel,\n",
       "\u001B[1;32m    310\u001B[0m     )\n",
       "\u001B[0;32m--> 311\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
       "\n",
       "File \u001B[0;32m/databricks/python/lib/python3.10/site-packages/pandas/core/reshape/concat.py:347\u001B[0m, in \u001B[0;36mconcat\u001B[0;34m(objs, axis, join, ignore_index, keys, levels, names, verify_integrity, sort, copy)\u001B[0m\n",
       "\u001B[1;32m    143\u001B[0m \u001B[38;5;129m@deprecate_nonkeyword_arguments\u001B[39m(version\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m, allowed_args\u001B[38;5;241m=\u001B[39m[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mobjs\u001B[39m\u001B[38;5;124m\"\u001B[39m])\n",
       "\u001B[1;32m    144\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mconcat\u001B[39m(\n",
       "\u001B[1;32m    145\u001B[0m     objs: Iterable[NDFrame] \u001B[38;5;241m|\u001B[39m Mapping[Hashable, NDFrame],\n",
       "\u001B[0;32m   (...)\u001B[0m\n",
       "\u001B[1;32m    154\u001B[0m     copy: \u001B[38;5;28mbool\u001B[39m \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m,\n",
       "\u001B[1;32m    155\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m DataFrame \u001B[38;5;241m|\u001B[39m Series:\n",
       "\u001B[1;32m    156\u001B[0m     \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n",
       "\u001B[1;32m    157\u001B[0m \u001B[38;5;124;03m    Concatenate pandas objects along a particular axis with optional set logic\u001B[39;00m\n",
       "\u001B[1;32m    158\u001B[0m \u001B[38;5;124;03m    along the other axes.\u001B[39;00m\n",
       "\u001B[0;32m   (...)\u001B[0m\n",
       "\u001B[1;32m    345\u001B[0m \u001B[38;5;124;03m    ValueError: Indexes have overlapping values: ['a']\u001B[39;00m\n",
       "\u001B[1;32m    346\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n",
       "\u001B[0;32m--> 347\u001B[0m     op \u001B[38;5;241m=\u001B[39m \u001B[43m_Concatenator\u001B[49m\u001B[43m(\u001B[49m\n",
       "\u001B[1;32m    348\u001B[0m \u001B[43m        \u001B[49m\u001B[43mobjs\u001B[49m\u001B[43m,\u001B[49m\n",
       "\u001B[1;32m    349\u001B[0m \u001B[43m        \u001B[49m\u001B[43maxis\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43maxis\u001B[49m\u001B[43m,\u001B[49m\n",
       "\u001B[1;32m    350\u001B[0m \u001B[43m        \u001B[49m\u001B[43mignore_index\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mignore_index\u001B[49m\u001B[43m,\u001B[49m\n",
       "\u001B[1;32m    351\u001B[0m \u001B[43m        \u001B[49m\u001B[43mjoin\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mjoin\u001B[49m\u001B[43m,\u001B[49m\n",
       "\u001B[1;32m    352\u001B[0m \u001B[43m        \u001B[49m\u001B[43mkeys\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mkeys\u001B[49m\u001B[43m,\u001B[49m\n",
       "\u001B[1;32m    353\u001B[0m \u001B[43m        \u001B[49m\u001B[43mlevels\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mlevels\u001B[49m\u001B[43m,\u001B[49m\n",
       "\u001B[1;32m    354\u001B[0m \u001B[43m        \u001B[49m\u001B[43mnames\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mnames\u001B[49m\u001B[43m,\u001B[49m\n",
       "\u001B[1;32m    355\u001B[0m \u001B[43m        \u001B[49m\u001B[43mverify_integrity\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mverify_integrity\u001B[49m\u001B[43m,\u001B[49m\n",
       "\u001B[1;32m    356\u001B[0m \u001B[43m        \u001B[49m\u001B[43mcopy\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcopy\u001B[49m\u001B[43m,\u001B[49m\n",
       "\u001B[1;32m    357\u001B[0m \u001B[43m        \u001B[49m\u001B[43msort\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43msort\u001B[49m\u001B[43m,\u001B[49m\n",
       "\u001B[1;32m    358\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n",
       "\u001B[1;32m    360\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m op\u001B[38;5;241m.\u001B[39mget_result()\n",
       "\n",
       "File \u001B[0;32m/databricks/python/lib/python3.10/site-packages/pandas/core/reshape/concat.py:437\u001B[0m, in \u001B[0;36m_Concatenator.__init__\u001B[0;34m(self, objs, axis, join, keys, levels, names, ignore_index, verify_integrity, copy, sort)\u001B[0m\n",
       "\u001B[1;32m    432\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(obj, (ABCSeries, ABCDataFrame)):\n",
       "\u001B[1;32m    433\u001B[0m         msg \u001B[38;5;241m=\u001B[39m (\n",
       "\u001B[1;32m    434\u001B[0m             \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcannot concatenate object of type \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mtype\u001B[39m(obj)\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m; \u001B[39m\u001B[38;5;124m\"\u001B[39m\n",
       "\u001B[1;32m    435\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124monly Series and DataFrame objs are valid\u001B[39m\u001B[38;5;124m\"\u001B[39m\n",
       "\u001B[1;32m    436\u001B[0m         )\n",
       "\u001B[0;32m--> 437\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mTypeError\u001B[39;00m(msg)\n",
       "\u001B[1;32m    439\u001B[0m     ndims\u001B[38;5;241m.\u001B[39madd(obj\u001B[38;5;241m.\u001B[39mndim)\n",
       "\u001B[1;32m    441\u001B[0m \u001B[38;5;66;03m# get the sample\u001B[39;00m\n",
       "\u001B[1;32m    442\u001B[0m \u001B[38;5;66;03m# want the highest ndim that we have, and must be non-empty\u001B[39;00m\n",
       "\u001B[1;32m    443\u001B[0m \u001B[38;5;66;03m# unless all objs are empty\u001B[39;00m\n",
       "\n",
       "\u001B[0;31mTypeError\u001B[0m: cannot concatenate object of type '<class 'dask.dataframe.core.DataFrame'>'; only Series and DataFrame objs are valid"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n\u001B[0;31mTypeError\u001B[0m                                 Traceback (most recent call last)\nFile \u001B[0;32m<command-2862209793706248>, line 53\u001B[0m\n\u001B[1;32m     51\u001B[0m parsed_ds \u001B[38;5;241m=\u001B[39m parsed_ds\u001B[38;5;241m.\u001B[39mreset_index(drop\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m)\n\u001B[1;32m     52\u001B[0m subset \u001B[38;5;241m=\u001B[39m subset\u001B[38;5;241m.\u001B[39mreset_index(drop\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m)\n\u001B[0;32m---> 53\u001B[0m final_df \u001B[38;5;241m=\u001B[39m pd\u001B[38;5;241m.\u001B[39mconcat([subset, parsed_ds], axis\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1\u001B[39m)\n\u001B[1;32m     54\u001B[0m \u001B[38;5;66;03m#final_df.reset_index(inplace=True)\u001B[39;00m\n\u001B[1;32m     55\u001B[0m final_df\u001B[38;5;241m.\u001B[39mto_parquet(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m/dbfs/FileStore/tables/processed_data_june_dask.parquet\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\nFile \u001B[0;32m/databricks/python/lib/python3.10/site-packages/pandas/util/_decorators.py:311\u001B[0m, in \u001B[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m    305\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(args) \u001B[38;5;241m>\u001B[39m num_allow_args:\n\u001B[1;32m    306\u001B[0m     warnings\u001B[38;5;241m.\u001B[39mwarn(\n\u001B[1;32m    307\u001B[0m         msg\u001B[38;5;241m.\u001B[39mformat(arguments\u001B[38;5;241m=\u001B[39marguments),\n\u001B[1;32m    308\u001B[0m         \u001B[38;5;167;01mFutureWarning\u001B[39;00m,\n\u001B[1;32m    309\u001B[0m         stacklevel\u001B[38;5;241m=\u001B[39mstacklevel,\n\u001B[1;32m    310\u001B[0m     )\n\u001B[0;32m--> 311\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\nFile \u001B[0;32m/databricks/python/lib/python3.10/site-packages/pandas/core/reshape/concat.py:347\u001B[0m, in \u001B[0;36mconcat\u001B[0;34m(objs, axis, join, ignore_index, keys, levels, names, verify_integrity, sort, copy)\u001B[0m\n\u001B[1;32m    143\u001B[0m \u001B[38;5;129m@deprecate_nonkeyword_arguments\u001B[39m(version\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m, allowed_args\u001B[38;5;241m=\u001B[39m[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mobjs\u001B[39m\u001B[38;5;124m\"\u001B[39m])\n\u001B[1;32m    144\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mconcat\u001B[39m(\n\u001B[1;32m    145\u001B[0m     objs: Iterable[NDFrame] \u001B[38;5;241m|\u001B[39m Mapping[Hashable, NDFrame],\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    154\u001B[0m     copy: \u001B[38;5;28mbool\u001B[39m \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m,\n\u001B[1;32m    155\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m DataFrame \u001B[38;5;241m|\u001B[39m Series:\n\u001B[1;32m    156\u001B[0m     \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m    157\u001B[0m \u001B[38;5;124;03m    Concatenate pandas objects along a particular axis with optional set logic\u001B[39;00m\n\u001B[1;32m    158\u001B[0m \u001B[38;5;124;03m    along the other axes.\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    345\u001B[0m \u001B[38;5;124;03m    ValueError: Indexes have overlapping values: ['a']\u001B[39;00m\n\u001B[1;32m    346\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[0;32m--> 347\u001B[0m     op \u001B[38;5;241m=\u001B[39m \u001B[43m_Concatenator\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    348\u001B[0m \u001B[43m        \u001B[49m\u001B[43mobjs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    349\u001B[0m \u001B[43m        \u001B[49m\u001B[43maxis\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43maxis\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    350\u001B[0m \u001B[43m        \u001B[49m\u001B[43mignore_index\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mignore_index\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    351\u001B[0m \u001B[43m        \u001B[49m\u001B[43mjoin\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mjoin\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    352\u001B[0m \u001B[43m        \u001B[49m\u001B[43mkeys\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mkeys\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    353\u001B[0m \u001B[43m        \u001B[49m\u001B[43mlevels\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mlevels\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    354\u001B[0m \u001B[43m        \u001B[49m\u001B[43mnames\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mnames\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    355\u001B[0m \u001B[43m        \u001B[49m\u001B[43mverify_integrity\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mverify_integrity\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    356\u001B[0m \u001B[43m        \u001B[49m\u001B[43mcopy\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcopy\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    357\u001B[0m \u001B[43m        \u001B[49m\u001B[43msort\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43msort\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    358\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    360\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m op\u001B[38;5;241m.\u001B[39mget_result()\n\nFile \u001B[0;32m/databricks/python/lib/python3.10/site-packages/pandas/core/reshape/concat.py:437\u001B[0m, in \u001B[0;36m_Concatenator.__init__\u001B[0;34m(self, objs, axis, join, keys, levels, names, ignore_index, verify_integrity, copy, sort)\u001B[0m\n\u001B[1;32m    432\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(obj, (ABCSeries, ABCDataFrame)):\n\u001B[1;32m    433\u001B[0m         msg \u001B[38;5;241m=\u001B[39m (\n\u001B[1;32m    434\u001B[0m             \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcannot concatenate object of type \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mtype\u001B[39m(obj)\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m; \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    435\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124monly Series and DataFrame objs are valid\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    436\u001B[0m         )\n\u001B[0;32m--> 437\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mTypeError\u001B[39;00m(msg)\n\u001B[1;32m    439\u001B[0m     ndims\u001B[38;5;241m.\u001B[39madd(obj\u001B[38;5;241m.\u001B[39mndim)\n\u001B[1;32m    441\u001B[0m \u001B[38;5;66;03m# get the sample\u001B[39;00m\n\u001B[1;32m    442\u001B[0m \u001B[38;5;66;03m# want the highest ndim that we have, and must be non-empty\u001B[39;00m\n\u001B[1;32m    443\u001B[0m \u001B[38;5;66;03m# unless all objs are empty\u001B[39;00m\n\n\u001B[0;31mTypeError\u001B[0m: cannot concatenate object of type '<class 'dask.dataframe.core.DataFrame'>'; only Series and DataFrame objs are valid",
       "errorSummary": "<span class='ansi-red-fg'>TypeError</span>: cannot concatenate object of type '<class 'dask.dataframe.core.DataFrame'>'; only Series and DataFrame objs are valid",
       "errorTraceType": "ansi",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import re\n",
    "keys = ['TAGS', 'ACTION', 'OBJECT', 'TEST DATA', 'DB DATA', 'PRIOR_ACTIONS', 'ROOT CAUSE'] #'PLATFORM', 'CAPABILITIES', 'RESOLUTION']\n",
    "\n",
    "def extract_values(text, key):\n",
    "    pattern = re.compile(rf'{key}:\\s*(.*?)(?:\\n|$|▐)')\n",
    "    match = pattern.search(text)\n",
    "    return match.group(1).strip() if match else None\n",
    "\n",
    "def split_text(keys, data):\n",
    "    result = {k: [] for k in keys}\n",
    "    for idx, ds in enumerate(data[\"StepDescription\"]):\n",
    "        for key in keys:\n",
    "            result[key].append(extract_values(ds, key))\n",
    "    return pd.DataFrame(result)\n",
    "\n",
    "def split_text_1(data):\n",
    "    result = {k: [] for k in keys}\n",
    "    print(data.columns)\n",
    "    for idx, ds in enumerate(data[\"StepDescription\"]):\n",
    "        for line in ds.split('\\n'):\n",
    "            split_div = line.split('▐')\n",
    "            if len(split_div) > 1:\n",
    "                for s in split_div:\n",
    "                    parts = s.split(':')\n",
    "                    k = parts[0].strip()\n",
    "                    if k in keys:\n",
    "                        v = parts[1].strip()\n",
    "                        result[k].append(v)\n",
    "                    if k== 'UTILITY ACTION':\n",
    "                        result['ACTION'].append(parts[1].strip())\n",
    "\n",
    "            else:\n",
    "                parts = line.split(':')\n",
    "                k = parts[0].strip()\n",
    "                if k in keys:\n",
    "                    v = ''.join(parts[1:]).strip()\n",
    "                    result[k].append(v)\n",
    "        for k in keys:\n",
    "            if len(result[k]) < idx + 1:\n",
    "                result[k].append(np.nan)\n",
    "    return pd.DataFrame(result)\n",
    "\n",
    "#print(split_text(df.StepDescription[1]))\n",
    "\n",
    "parsed_ds = split_text(keys, df)\n",
    "subset = df.drop(columns=['StepDescription'])\n",
    "print(\"PARSED: \", parsed_ds.shape[0])\n",
    "print(\"SUBSET: \", subset.shape[0])\n",
    "#subset['TAGS'] = parsed_ds['TAGS']\n",
    "parsed_ds = parsed_ds.reset_index(drop=True)\n",
    "subset = subset.reset_index(drop=True)\n",
    "final_df = pd.concat([subset, parsed_ds], axis=1)\n",
    "#final_df.reset_index(inplace=True)\n",
    "final_df.to_parquet('/dbfs/FileStore/tables/processed_data_june_dask.parquet')\n",
    "#final_df = final_df[final_df['ExecutionItemID'].isna()]\n",
    "#final_df\n",
    "final_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ba909228-103e-45e9-8396-f2dfb67c0d25",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n",
       "\u001B[0;31mValueError\u001B[0m                                Traceback (most recent call last)\n",
       "File \u001B[0;32m<command-1011379387708872>, line 14\u001B[0m\n",
       "\u001B[1;32m     12\u001B[0m ddf_enc \u001B[38;5;241m=\u001B[39m ddf_enc\u001B[38;5;241m.\u001B[39mdropna()\n",
       "\u001B[1;32m     13\u001B[0m \u001B[38;5;66;03m# Weighted Random Sampler for class imbalance\u001B[39;00m\n",
       "\u001B[0;32m---> 14\u001B[0m \u001B[43mddf_enc\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mclass_weight\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m]\u001B[49m \u001B[38;5;241m=\u001B[39m pd\u001B[38;5;241m.\u001B[39mSeries(compute_sample_weight(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mbalanced\u001B[39m\u001B[38;5;124m'\u001B[39m, ddf_enc[label_col]))\n",
       "\u001B[1;32m     15\u001B[0m ddf_enc\n",
       "\n",
       "File \u001B[0;32m/local_disk0/.ephemeral_nfs/envs/pythonEnv-fa9614d8-7e9c-40b7-9e5e-09b1f38d56f7/lib/python3.10/site-packages/dask/dataframe/core.py:4864\u001B[0m, in \u001B[0;36mDataFrame.__setitem__\u001B[0;34m(self, key, value)\u001B[0m\n",
       "\u001B[1;32m   4862\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mNotImplementedError\u001B[39;00m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mItem assignment with \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mtype\u001B[39m(key)\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m not supported\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
       "\u001B[1;32m   4863\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n",
       "\u001B[0;32m-> 4864\u001B[0m     df \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43massign\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43m{\u001B[49m\u001B[43mkey\u001B[49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mvalue\u001B[49m\u001B[43m}\u001B[49m\u001B[43m)\u001B[49m\n",
       "\u001B[1;32m   4866\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdask \u001B[38;5;241m=\u001B[39m df\u001B[38;5;241m.\u001B[39mdask\n",
       "\u001B[1;32m   4867\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_name \u001B[38;5;241m=\u001B[39m df\u001B[38;5;241m.\u001B[39m_name\n",
       "\n",
       "File \u001B[0;32m/local_disk0/.ephemeral_nfs/envs/pythonEnv-fa9614d8-7e9c-40b7-9e5e-09b1f38d56f7/lib/python3.10/site-packages/dask/dataframe/core.py:5305\u001B[0m, in \u001B[0;36mDataFrame.assign\u001B[0;34m(self, **kwargs)\u001B[0m\n",
       "\u001B[1;32m   5301\u001B[0m     \u001B[38;5;66;03m# Figure out columns of the output\u001B[39;00m\n",
       "\u001B[1;32m   5302\u001B[0m     df2 \u001B[38;5;241m=\u001B[39m data\u001B[38;5;241m.\u001B[39m_meta_nonempty\u001B[38;5;241m.\u001B[39massign(\n",
       "\u001B[1;32m   5303\u001B[0m         \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39m_extract_meta({k: kwargs[k]}, nonempty\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m)\n",
       "\u001B[1;32m   5304\u001B[0m     )\n",
       "\u001B[0;32m-> 5305\u001B[0m     data \u001B[38;5;241m=\u001B[39m \u001B[43melemwise\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmethods\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43massign\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdata\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mpairs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmeta\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdf2\u001B[49m\u001B[43m)\u001B[49m\n",
       "\u001B[1;32m   5307\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m data\n",
       "\n",
       "File \u001B[0;32m/local_disk0/.ephemeral_nfs/envs/pythonEnv-fa9614d8-7e9c-40b7-9e5e-09b1f38d56f7/lib/python3.10/site-packages/dask/dataframe/core.py:6405\u001B[0m, in \u001B[0;36melemwise\u001B[0;34m(op, meta, out, transform_divisions, *args, **kwargs)\u001B[0m\n",
       "\u001B[1;32m   6401\u001B[0m args \u001B[38;5;241m=\u001B[39m _maybe_from_pandas(args)\n",
       "\u001B[1;32m   6403\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mdask\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mdataframe\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mmulti\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m _maybe_align_partitions\n",
       "\u001B[0;32m-> 6405\u001B[0m args \u001B[38;5;241m=\u001B[39m \u001B[43m_maybe_align_partitions\u001B[49m\u001B[43m(\u001B[49m\u001B[43margs\u001B[49m\u001B[43m)\u001B[49m\n",
       "\u001B[1;32m   6406\u001B[0m dasks \u001B[38;5;241m=\u001B[39m [arg \u001B[38;5;28;01mfor\u001B[39;00m arg \u001B[38;5;129;01min\u001B[39;00m args \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(arg, (_Frame, Scalar, Array))]\n",
       "\u001B[1;32m   6407\u001B[0m dfs \u001B[38;5;241m=\u001B[39m [df \u001B[38;5;28;01mfor\u001B[39;00m df \u001B[38;5;129;01min\u001B[39;00m dasks \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(df, _Frame)]\n",
       "\n",
       "File \u001B[0;32m/local_disk0/.ephemeral_nfs/envs/pythonEnv-fa9614d8-7e9c-40b7-9e5e-09b1f38d56f7/lib/python3.10/site-packages/dask/dataframe/multi.py:174\u001B[0m, in \u001B[0;36m_maybe_align_partitions\u001B[0;34m(args)\u001B[0m\n",
       "\u001B[1;32m    172\u001B[0m divisions \u001B[38;5;241m=\u001B[39m dfs[\u001B[38;5;241m0\u001B[39m]\u001B[38;5;241m.\u001B[39mdivisions\n",
       "\u001B[1;32m    173\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28mall\u001B[39m(df\u001B[38;5;241m.\u001B[39mdivisions \u001B[38;5;241m==\u001B[39m divisions \u001B[38;5;28;01mfor\u001B[39;00m df \u001B[38;5;129;01min\u001B[39;00m dfs):\n",
       "\u001B[0;32m--> 174\u001B[0m     dfs2 \u001B[38;5;241m=\u001B[39m \u001B[38;5;28miter\u001B[39m(\u001B[43malign_partitions\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mdfs\u001B[49m\u001B[43m)\u001B[49m[\u001B[38;5;241m0\u001B[39m])\n",
       "\u001B[1;32m    175\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m [a \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(a, _Frame) \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28mnext\u001B[39m(dfs2) \u001B[38;5;28;01mfor\u001B[39;00m a \u001B[38;5;129;01min\u001B[39;00m args]\n",
       "\u001B[1;32m    176\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m args\n",
       "\n",
       "File \u001B[0;32m/local_disk0/.ephemeral_nfs/envs/pythonEnv-fa9614d8-7e9c-40b7-9e5e-09b1f38d56f7/lib/python3.10/site-packages/dask/dataframe/multi.py:128\u001B[0m, in \u001B[0;36malign_partitions\u001B[0;34m(*dfs)\u001B[0m\n",
       "\u001B[1;32m    126\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mdfs contains no DataFrame and Series\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
       "\u001B[1;32m    127\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28mall\u001B[39m(df\u001B[38;5;241m.\u001B[39mknown_divisions \u001B[38;5;28;01mfor\u001B[39;00m df \u001B[38;5;129;01min\u001B[39;00m dfs1):\n",
       "\u001B[0;32m--> 128\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n",
       "\u001B[1;32m    129\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mNot all divisions are known, can\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mt align \u001B[39m\u001B[38;5;124m\"\u001B[39m\n",
       "\u001B[1;32m    130\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mpartitions. Please use `set_index` \u001B[39m\u001B[38;5;124m\"\u001B[39m\n",
       "\u001B[1;32m    131\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mto set the index.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n",
       "\u001B[1;32m    132\u001B[0m     )\n",
       "\u001B[1;32m    134\u001B[0m divisions \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mlist\u001B[39m(unique(merge_sorted(\u001B[38;5;241m*\u001B[39m[df\u001B[38;5;241m.\u001B[39mdivisions \u001B[38;5;28;01mfor\u001B[39;00m df \u001B[38;5;129;01min\u001B[39;00m dfs1])))\n",
       "\u001B[1;32m    135\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(divisions) \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m1\u001B[39m:  \u001B[38;5;66;03m# single value for index\u001B[39;00m\n",
       "\n",
       "\u001B[0;31mValueError\u001B[0m: Not all divisions are known, can't align partitions. Please use `set_index` to set the index."
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n\u001B[0;31mValueError\u001B[0m                                Traceback (most recent call last)\nFile \u001B[0;32m<command-1011379387708872>, line 14\u001B[0m\n\u001B[1;32m     12\u001B[0m ddf_enc \u001B[38;5;241m=\u001B[39m ddf_enc\u001B[38;5;241m.\u001B[39mdropna()\n\u001B[1;32m     13\u001B[0m \u001B[38;5;66;03m# Weighted Random Sampler for class imbalance\u001B[39;00m\n\u001B[0;32m---> 14\u001B[0m \u001B[43mddf_enc\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mclass_weight\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m]\u001B[49m \u001B[38;5;241m=\u001B[39m pd\u001B[38;5;241m.\u001B[39mSeries(compute_sample_weight(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mbalanced\u001B[39m\u001B[38;5;124m'\u001B[39m, ddf_enc[label_col]))\n\u001B[1;32m     15\u001B[0m ddf_enc\n\nFile \u001B[0;32m/local_disk0/.ephemeral_nfs/envs/pythonEnv-fa9614d8-7e9c-40b7-9e5e-09b1f38d56f7/lib/python3.10/site-packages/dask/dataframe/core.py:4864\u001B[0m, in \u001B[0;36mDataFrame.__setitem__\u001B[0;34m(self, key, value)\u001B[0m\n\u001B[1;32m   4862\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mNotImplementedError\u001B[39;00m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mItem assignment with \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mtype\u001B[39m(key)\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m not supported\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m   4863\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 4864\u001B[0m     df \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43massign\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43m{\u001B[49m\u001B[43mkey\u001B[49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mvalue\u001B[49m\u001B[43m}\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   4866\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdask \u001B[38;5;241m=\u001B[39m df\u001B[38;5;241m.\u001B[39mdask\n\u001B[1;32m   4867\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_name \u001B[38;5;241m=\u001B[39m df\u001B[38;5;241m.\u001B[39m_name\n\nFile \u001B[0;32m/local_disk0/.ephemeral_nfs/envs/pythonEnv-fa9614d8-7e9c-40b7-9e5e-09b1f38d56f7/lib/python3.10/site-packages/dask/dataframe/core.py:5305\u001B[0m, in \u001B[0;36mDataFrame.assign\u001B[0;34m(self, **kwargs)\u001B[0m\n\u001B[1;32m   5301\u001B[0m     \u001B[38;5;66;03m# Figure out columns of the output\u001B[39;00m\n\u001B[1;32m   5302\u001B[0m     df2 \u001B[38;5;241m=\u001B[39m data\u001B[38;5;241m.\u001B[39m_meta_nonempty\u001B[38;5;241m.\u001B[39massign(\n\u001B[1;32m   5303\u001B[0m         \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39m_extract_meta({k: kwargs[k]}, nonempty\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m)\n\u001B[1;32m   5304\u001B[0m     )\n\u001B[0;32m-> 5305\u001B[0m     data \u001B[38;5;241m=\u001B[39m \u001B[43melemwise\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmethods\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43massign\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdata\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mpairs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmeta\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdf2\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   5307\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m data\n\nFile \u001B[0;32m/local_disk0/.ephemeral_nfs/envs/pythonEnv-fa9614d8-7e9c-40b7-9e5e-09b1f38d56f7/lib/python3.10/site-packages/dask/dataframe/core.py:6405\u001B[0m, in \u001B[0;36melemwise\u001B[0;34m(op, meta, out, transform_divisions, *args, **kwargs)\u001B[0m\n\u001B[1;32m   6401\u001B[0m args \u001B[38;5;241m=\u001B[39m _maybe_from_pandas(args)\n\u001B[1;32m   6403\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mdask\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mdataframe\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mmulti\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m _maybe_align_partitions\n\u001B[0;32m-> 6405\u001B[0m args \u001B[38;5;241m=\u001B[39m \u001B[43m_maybe_align_partitions\u001B[49m\u001B[43m(\u001B[49m\u001B[43margs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   6406\u001B[0m dasks \u001B[38;5;241m=\u001B[39m [arg \u001B[38;5;28;01mfor\u001B[39;00m arg \u001B[38;5;129;01min\u001B[39;00m args \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(arg, (_Frame, Scalar, Array))]\n\u001B[1;32m   6407\u001B[0m dfs \u001B[38;5;241m=\u001B[39m [df \u001B[38;5;28;01mfor\u001B[39;00m df \u001B[38;5;129;01min\u001B[39;00m dasks \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(df, _Frame)]\n\nFile \u001B[0;32m/local_disk0/.ephemeral_nfs/envs/pythonEnv-fa9614d8-7e9c-40b7-9e5e-09b1f38d56f7/lib/python3.10/site-packages/dask/dataframe/multi.py:174\u001B[0m, in \u001B[0;36m_maybe_align_partitions\u001B[0;34m(args)\u001B[0m\n\u001B[1;32m    172\u001B[0m divisions \u001B[38;5;241m=\u001B[39m dfs[\u001B[38;5;241m0\u001B[39m]\u001B[38;5;241m.\u001B[39mdivisions\n\u001B[1;32m    173\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28mall\u001B[39m(df\u001B[38;5;241m.\u001B[39mdivisions \u001B[38;5;241m==\u001B[39m divisions \u001B[38;5;28;01mfor\u001B[39;00m df \u001B[38;5;129;01min\u001B[39;00m dfs):\n\u001B[0;32m--> 174\u001B[0m     dfs2 \u001B[38;5;241m=\u001B[39m \u001B[38;5;28miter\u001B[39m(\u001B[43malign_partitions\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mdfs\u001B[49m\u001B[43m)\u001B[49m[\u001B[38;5;241m0\u001B[39m])\n\u001B[1;32m    175\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m [a \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(a, _Frame) \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28mnext\u001B[39m(dfs2) \u001B[38;5;28;01mfor\u001B[39;00m a \u001B[38;5;129;01min\u001B[39;00m args]\n\u001B[1;32m    176\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m args\n\nFile \u001B[0;32m/local_disk0/.ephemeral_nfs/envs/pythonEnv-fa9614d8-7e9c-40b7-9e5e-09b1f38d56f7/lib/python3.10/site-packages/dask/dataframe/multi.py:128\u001B[0m, in \u001B[0;36malign_partitions\u001B[0;34m(*dfs)\u001B[0m\n\u001B[1;32m    126\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mdfs contains no DataFrame and Series\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m    127\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28mall\u001B[39m(df\u001B[38;5;241m.\u001B[39mknown_divisions \u001B[38;5;28;01mfor\u001B[39;00m df \u001B[38;5;129;01min\u001B[39;00m dfs1):\n\u001B[0;32m--> 128\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[1;32m    129\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mNot all divisions are known, can\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mt align \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    130\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mpartitions. Please use `set_index` \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    131\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mto set the index.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    132\u001B[0m     )\n\u001B[1;32m    134\u001B[0m divisions \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mlist\u001B[39m(unique(merge_sorted(\u001B[38;5;241m*\u001B[39m[df\u001B[38;5;241m.\u001B[39mdivisions \u001B[38;5;28;01mfor\u001B[39;00m df \u001B[38;5;129;01min\u001B[39;00m dfs1])))\n\u001B[1;32m    135\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(divisions) \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m1\u001B[39m:  \u001B[38;5;66;03m# single value for index\u001B[39;00m\n\n\u001B[0;31mValueError\u001B[0m: Not all divisions are known, can't align partitions. Please use `set_index` to set the index.",
       "errorSummary": "<span class='ansi-red-fg'>ValueError</span>: Not all divisions are known, can't align partitions. Please use `set_index` to set the index.",
       "errorTraceType": "ansi",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "if run_type == 'cpu':\n",
    "     ddf_enc = dd.read_parquet(DATA_PATH)\n",
    "elif run_type == 'gpu':\n",
    "    import dask_cudf\n",
    "    import cupy as cp\n",
    "    ddf_enc = dask_cudf.read_parquet(DATA_PATH)\n",
    "else:\n",
    "    raise ValueError('Select run_type of cpu or gpu.')\n",
    "#df = df.drop(columns=['RESOLUTION'])\n",
    "ddf_enc = ddf_enc[['TAGS', 'ACTION', 'OBJECT', 'PRIOR_ACTIONS', 'ROOT CAUSE', 'screenshot', 'IssueType', 'CAT']]\n",
    "ddf_enc = ddf_enc[ddf_enc['IssueType'] != 'PROCESSING_ISSUE'] \n",
    "ddf_enc = ddf_enc.dropna()\n",
    "# Weighted Random Sampler for class imbalance\n",
    "ddf_enc['class_weight'] = compute_sample_weight('balanced', ddf_enc[label_col]))\n",
    "ddf_enc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "99f203b8-6124-4eb2-ab6c-10648c201a2d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "Dask Series Structure:\n",
       "npartitions=1\n",
       "    int64\n",
       "      ...\n",
       "Name: ACTION, dtype: int64\n",
       "Dask Name: value-counts-agg, 9 graph layers"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ddf_enc.ACTION.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "672761ab-8fae-42f7-a97d-682887f59e65",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TAGS</th>\n",
       "      <th>ACTION</th>\n",
       "      <th>OBJECT</th>\n",
       "      <th>PRIOR_ACTIONS</th>\n",
       "      <th>ROOT CAUSE</th>\n",
       "      <th>screenshot</th>\n",
       "      <th>IssueType</th>\n",
       "      <th>CAT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>@us @regression @csn @DirectBook @DirectBook_B...</td>\n",
       "      <td>Input</td>\n",
       "      <td>DirectBook.Booking-GuestInfo.btnGuestName</td>\n",
       "      <td>Input | Input | Input | Input | Input</td>\n",
       "      <td>WaitTimeoutException: TIMED OUT AFTER 30 TOTAL...</td>\n",
       "      <td>b'\\x89PNG\\r\\n\\x1a\\n\\x00\\x00\\x00\\rIHDR\\x00\\x00\\...</td>\n",
       "      <td>Sync Issue</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>@us @regression @csn @DirectBook @DirectBook_B...</td>\n",
       "      <td>Input</td>\n",
       "      <td>DirectBook.Booking-VnuRoomDetails.lnkFirstRoom...</td>\n",
       "      <td>Input | Input | VerifyExistence | Input | Wait...</td>\n",
       "      <td>WaitTimeoutException: TIMED OUT AFTER 1 TOTAL ...</td>\n",
       "      <td>b'\\x89PNG\\r\\n\\x1a\\n\\x00\\x00\\x00\\rIHDR\\x00\\x00\\...</td>\n",
       "      <td>Sync Issue</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>@us @flex @regression @session_bundles @wax @a...</td>\n",
       "      <td>VerifyExistence</td>\n",
       "      <td>Flex.Guest_Side_Summary.txt-congratulations-co...</td>\n",
       "      <td>Input | VerifyExistence | VerifyExistence | In...</td>\n",
       "      <td>WaitTimeoutException: TIMED OUT AFTER 1 TOTAL ...</td>\n",
       "      <td>b'\\x89PNG\\r\\n\\x1a\\n\\x00\\x00\\x00\\rIHDR\\x00\\x00\\...</td>\n",
       "      <td>Sync Issue</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>@us @flex @regression @session_bundles @wax @a...</td>\n",
       "      <td>VerifyExistence</td>\n",
       "      <td>Flex.Guest_Side_Registration.txtRegSummaryPage...</td>\n",
       "      <td>WaitForSeconds | VerifyExistence | VerifyExist...</td>\n",
       "      <td>HardErrorException: Application Issue Found wi...</td>\n",
       "      <td>b'\\x89PNG\\r\\n\\x1a\\n\\x00\\x00\\x00\\rIHDR\\x00\\x00\\...</td>\n",
       "      <td>Sync Issue</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>@us @regression @csn @DirectBook @DirectBook_B...</td>\n",
       "      <td>Output</td>\n",
       "      <td>DirectBook.Booking-LandingPage.txtDropDownName</td>\n",
       "      <td>Input |  | Input | Input | Output</td>\n",
       "      <td>WaitTimeoutException: TIMED OUT AFTER 1 TOTAL ...</td>\n",
       "      <td>b'\\x89PNG\\r\\n\\x1a\\n\\x00\\x00\\x00\\rIHDR\\x00\\x00\\...</td>\n",
       "      <td>Sync Issue</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                TAGS  ... CAT\n",
       "0  @us @regression @csn @DirectBook @DirectBook_B...  ...   0\n",
       "1  @us @regression @csn @DirectBook @DirectBook_B...  ...   0\n",
       "2  @us @flex @regression @session_bundles @wax @a...  ...   0\n",
       "3  @us @flex @regression @session_bundles @wax @a...  ...   0\n",
       "5  @us @regression @csn @DirectBook @DirectBook_B...  ...   0\n",
       "\n",
       "[5 rows x 8 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Label encode actions/prior actions\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "#label_enc = LabelEncoder()\n",
    "#df['ACTION_Enc'] = label_enc.fit_transform(df[\"ACTION\"])\n",
    "#df['PRIOR_ACTIONS_SPLIT'] = df['PRIOR_ACTIONS'].apply(lambda entry: entry.split('|'))\n",
    "ddf_enc.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a84c0f37-1967-4d0e-a92d-42312111a067",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n",
       "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)\n",
       "File \u001B[0;32m<command-1713639788819841>, line 2\u001B[0m\n",
       "\u001B[1;32m      1\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01msklearn\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mmodel_selection\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m train_test_split\n",
       "\u001B[0;32m----> 2\u001B[0m df[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mclass_weight\u001B[39m\u001B[38;5;124m'\u001B[39m] \u001B[38;5;241m=\u001B[39m compute_sample_weight(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mbalanced\u001B[39m\u001B[38;5;124m'\u001B[39m, df[label_col])\n",
       "\u001B[1;32m      3\u001B[0m train_dataset, test_dataset \u001B[38;5;241m=\u001B[39m train_test_split(\n",
       "\u001B[1;32m      4\u001B[0m     ddf_enc, train_size\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m0.8\u001B[39m, random_state\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m42\u001B[39m, stratify\u001B[38;5;241m=\u001B[39mdf\u001B[38;5;241m.\u001B[39mCAT\n",
       "\u001B[1;32m      5\u001B[0m )\n",
       "\u001B[1;32m      6\u001B[0m train_dataset \u001B[38;5;241m=\u001B[39m train_dataset\u001B[38;5;241m.\u001B[39mreset_index(drop\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m)\n",
       "\n",
       "\u001B[0;31mNameError\u001B[0m: name 'df' is not defined"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)\nFile \u001B[0;32m<command-1713639788819841>, line 2\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01msklearn\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mmodel_selection\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m train_test_split\n\u001B[0;32m----> 2\u001B[0m df[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mclass_weight\u001B[39m\u001B[38;5;124m'\u001B[39m] \u001B[38;5;241m=\u001B[39m compute_sample_weight(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mbalanced\u001B[39m\u001B[38;5;124m'\u001B[39m, df[label_col])\n\u001B[1;32m      3\u001B[0m train_dataset, test_dataset \u001B[38;5;241m=\u001B[39m train_test_split(\n\u001B[1;32m      4\u001B[0m     ddf_enc, train_size\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m0.8\u001B[39m, random_state\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m42\u001B[39m, stratify\u001B[38;5;241m=\u001B[39mdf\u001B[38;5;241m.\u001B[39mCAT\n\u001B[1;32m      5\u001B[0m )\n\u001B[1;32m      6\u001B[0m train_dataset \u001B[38;5;241m=\u001B[39m train_dataset\u001B[38;5;241m.\u001B[39mreset_index(drop\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m)\n\n\u001B[0;31mNameError\u001B[0m: name 'df' is not defined",
       "errorSummary": "<span class='ansi-red-fg'>NameError</span>: name 'df' is not defined",
       "errorTraceType": "ansi",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "df['class_weight'] = compute_sample_weight('balanced', df[label_col])\n",
    "train_dataset, test_dataset = train_test_split(\n",
    "    ddf_enc, train_size=0.8, random_state=42, stratify=df.CAT\n",
    ")\n",
    "train_dataset = train_dataset.reset_index(drop=True)\n",
    "# train_dataset = train_dataset[train_dataset['IssueType'] != \"SYNC_ISSUE\"]\n",
    "test_dataset = test_dataset.reset_index(drop=True)\n",
    "print(\"- TRAIN -\\n\")\n",
    "print(train_dataset.groupby([\"CAT\", \"IssueType\"])[\"CAT\"].count())\n",
    "print(\"-----------------\")\n",
    "print(\"- TEST -\\n\")\n",
    "print(test_dataset.groupby([\"CAT\", \"IssueType\"])[\"CAT\"].count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9a7078fc-a88a-45df-a789-4f42dfdb0514",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Set CV Params\n",
    "\n",
    "Hyperparam ranges cloned from Job Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e17e66c4-6cdc-4ab0-b931-36a2e48e7871",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def get_params():\n",
    "    learning_rate = st.uniform(0.05, 0.4)\n",
    "    max_depth = [10, 20, 50, 100, 200, 300, 500]\n",
    "    n_estimators = [5, 10, 25, 50, 75, 100, 200, 400]\n",
    "    reg_lambda = [0.1, 0.3, 1]\n",
    "    colsample_bytree = st.beta(10, 1)\n",
    "    gamma = st.uniform(0, 10)\n",
    "    subsample = [0.6, 0.8, 1.0]\n",
    "    reg_alpha = st.expon(0, 50)\n",
    "    min_child_weight = [1, 5, 10]\n",
    "    # Create the random grid\n",
    "    return {\n",
    "        \"vectorizer__analyzer\": [\"word\"],\n",
    "        \"vectorizer__stop_words\": [\"english\"],\n",
    "        \"vectorizer__strip_accents\": [\"ascii\"],\n",
    "        \"vectorizer__ngram_range\": [(1, 1), (1, 2), (2, 2)],\n",
    "        \"classifier__n_estimators\": n_estimators,\n",
    "        \"classifier__learning_rate\": learning_rate,\n",
    "        \"classifier__reg_lambda\": reg_lambda,\n",
    "        \"classifier__colsample_bytree\": colsample_bytree,\n",
    "        \"classifier__gamma\": gamma,\n",
    "        \"classifier__subsample\": subsample,\n",
    "        \"classifier__reg_alpha\": reg_alpha,\n",
    "        \"classifier__max_depth\": max_depth,\n",
    "        \"classifier__min_child_weight\": min_child_weight,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e66e21c7-ce10-4fda-8bd7-8ba1964466bb",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "66bf8f6f-e9ee-4412-a734-f3267bc93b25",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Define model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d1fdc895-dfa8-4b86-85b2-98cc7821a934",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "745212fa-d1a0-4bae-9e63-56646fd9d4fa",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n",
       "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)\n",
       "File \u001B[0;32m<command-1049894031074854>, line 88\u001B[0m\n",
       "\u001B[1;32m     77\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m(metrics)\n",
       "\u001B[1;32m     80\u001B[0m \u001B[38;5;66;03m# xgboost params:\u001B[39;00m\n",
       "\u001B[1;32m     81\u001B[0m xgb_params \u001B[38;5;241m=\u001B[39m { \n",
       "\u001B[1;32m     82\u001B[0m     \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mrandom_state\u001B[39m\u001B[38;5;124m'\u001B[39m: \u001B[38;5;241m0\u001B[39m,\n",
       "\u001B[1;32m     83\u001B[0m     \u001B[38;5;124m'\u001B[39m\u001B[38;5;124meta\u001B[39m\u001B[38;5;124m'\u001B[39m: \u001B[38;5;241m0.1\u001B[39m,\n",
       "\u001B[1;32m     84\u001B[0m     \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mgamma\u001B[39m\u001B[38;5;124m'\u001B[39m: \u001B[38;5;241m0.1\u001B[39m,\n",
       "\u001B[1;32m     85\u001B[0m     \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mmax_depth\u001B[39m\u001B[38;5;124m'\u001B[39m: max_depth, \n",
       "\u001B[1;32m     86\u001B[0m \u001B[38;5;66;03m#     'num_boost_round': num_boost_round, # Number of boosting rounds: num_boost_round in xgboost, numRound in xgboost4j, n_estimators in scikit-learn.\u001B[39;00m\n",
       "\u001B[1;32m     87\u001B[0m     \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mmax_leaves\u001B[39m\u001B[38;5;124m'\u001B[39m: \u001B[38;5;241m4\u001B[39m\u001B[38;5;241m*\u001B[39m\u001B[38;5;241m256\u001B[39m, \u001B[38;5;66;03m# Maximum number of nodes to be added. Only relevant when grow_policy=lossguide is set.\u001B[39;00m\n",
       "\u001B[0;32m---> 88\u001B[0m     \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mobjective\u001B[39m\u001B[38;5;124m'\u001B[39m: xgb_objective,\n",
       "\u001B[1;32m     89\u001B[0m     \u001B[38;5;124m'\u001B[39m\u001B[38;5;124meval_metric\u001B[39m\u001B[38;5;124m'\u001B[39m: \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mauc\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;66;03m# Applied to eval_set/test_data if provided\u001B[39;00m\n",
       "\u001B[1;32m     90\u001B[0m     \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mbooster\u001B[39m\u001B[38;5;124m'\u001B[39m: \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mgbtree\u001B[39m\u001B[38;5;124m'\u001B[39m,\n",
       "\u001B[1;32m     91\u001B[0m }\n",
       "\u001B[1;32m     93\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m run_type \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mcpu\u001B[39m\u001B[38;5;124m'\u001B[39m:\n",
       "\u001B[1;32m     94\u001B[0m     tree_method \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mhist\u001B[39m\u001B[38;5;124m'\u001B[39m\n",
       "\n",
       "\u001B[0;31mNameError\u001B[0m: name 'xgb_objective' is not defined"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)\nFile \u001B[0;32m<command-1049894031074854>, line 88\u001B[0m\n\u001B[1;32m     77\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m(metrics)\n\u001B[1;32m     80\u001B[0m \u001B[38;5;66;03m# xgboost params:\u001B[39;00m\n\u001B[1;32m     81\u001B[0m xgb_params \u001B[38;5;241m=\u001B[39m { \n\u001B[1;32m     82\u001B[0m     \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mrandom_state\u001B[39m\u001B[38;5;124m'\u001B[39m: \u001B[38;5;241m0\u001B[39m,\n\u001B[1;32m     83\u001B[0m     \u001B[38;5;124m'\u001B[39m\u001B[38;5;124meta\u001B[39m\u001B[38;5;124m'\u001B[39m: \u001B[38;5;241m0.1\u001B[39m,\n\u001B[1;32m     84\u001B[0m     \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mgamma\u001B[39m\u001B[38;5;124m'\u001B[39m: \u001B[38;5;241m0.1\u001B[39m,\n\u001B[1;32m     85\u001B[0m     \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mmax_depth\u001B[39m\u001B[38;5;124m'\u001B[39m: max_depth, \n\u001B[1;32m     86\u001B[0m \u001B[38;5;66;03m#     'num_boost_round': num_boost_round, # Number of boosting rounds: num_boost_round in xgboost, numRound in xgboost4j, n_estimators in scikit-learn.\u001B[39;00m\n\u001B[1;32m     87\u001B[0m     \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mmax_leaves\u001B[39m\u001B[38;5;124m'\u001B[39m: \u001B[38;5;241m4\u001B[39m\u001B[38;5;241m*\u001B[39m\u001B[38;5;241m256\u001B[39m, \u001B[38;5;66;03m# Maximum number of nodes to be added. Only relevant when grow_policy=lossguide is set.\u001B[39;00m\n\u001B[0;32m---> 88\u001B[0m     \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mobjective\u001B[39m\u001B[38;5;124m'\u001B[39m: xgb_objective,\n\u001B[1;32m     89\u001B[0m     \u001B[38;5;124m'\u001B[39m\u001B[38;5;124meval_metric\u001B[39m\u001B[38;5;124m'\u001B[39m: \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mauc\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;66;03m# Applied to eval_set/test_data if provided\u001B[39;00m\n\u001B[1;32m     90\u001B[0m     \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mbooster\u001B[39m\u001B[38;5;124m'\u001B[39m: \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mgbtree\u001B[39m\u001B[38;5;124m'\u001B[39m,\n\u001B[1;32m     91\u001B[0m }\n\u001B[1;32m     93\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m run_type \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mcpu\u001B[39m\u001B[38;5;124m'\u001B[39m:\n\u001B[1;32m     94\u001B[0m     tree_method \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mhist\u001B[39m\u001B[38;5;124m'\u001B[39m\n\n\u001B[0;31mNameError\u001B[0m: name 'xgb_objective' is not defined",
       "errorSummary": "<span class='ansi-red-fg'>NameError</span>: name 'xgb_objective' is not defined",
       "errorTraceType": "ansi",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def dask_xgb_train(client, run_type, ddf_enc, label_col, xgb_params, xgb_model_name):\n",
    "    \"\"\"\n",
    "    Use dask + xgboost for training against already encoded data.\n",
    "    \"\"\"\n",
    "    tic = time()\n",
    "    ddf_enc = client.persist(ddf_enc)\n",
    "#     wait([ddf_enc])\n",
    "\n",
    "    feature_cols = [cc for cc in ddf_enc.columns if cc in included_features]\n",
    "    print('Feature columns:', feature_cols)\n",
    "    print('  Number of feature columns:', len(feature_cols))\n",
    "\n",
    "   # preprocessor = ColumnTransformer(\n",
    "   #  transformers=[\n",
    "   #      ('text', TfidfVectorizer(), 'raw_text_ft'), #TfidfVectorizer accepts column name only between quotes\n",
    "   #      ('category', OneHotEncoder(), ['categorical_ft']),\n",
    "   #  ],\n",
    "   # )\n",
    "\n",
    "    vectorizer = CountVectorizer()\n",
    "    encoder = OneHotEncoder()\n",
    "    mlb = MultiLabelBinarizer()\n",
    "    X_text = vectorizer.fit_transform(ddf_enc['TAGS'] + ' ' + ddf_enc['OBJECT'] + ' ' + ddf_enc['ROOT CAUSE']).toarray()\n",
    "    X_ohe = encoder.fit_transform(ddf_enc[[\"ACTION\"]]).toarray()\n",
    "    X_prior_actions = mlb.fit_transform(ddf_enc['PRIOR_ACTIONS'].split('|')).toarray()\n",
    "    X = np.hstack((X_test, X_ohe, X_prior_actions))\n",
    "\n",
    "    #X = ddf_enc[feature_cols].astype('float32')\n",
    "    y = ddf_enc[label_col].astype('float32')\n",
    "    \n",
    "    # Per-instance weight required due to severe class imbalance in One-vs-Rest decomposition.\n",
    "    # Weights precalculated during ETL on training set alone to avoid leakage.\n",
    "    weight = ddf_enc['class_weight'].astype('float32')\n",
    "    \n",
    "#     # Persist data:\n",
    "#     X = client.persist(X)\n",
    "#     y = client.persist(y)\n",
    "#     weight = client.persist(weight)\n",
    "#     wait([X, y, weight])\n",
    "\n",
    "    # Use XGBOOST API:\n",
    "    if run_type == 'gpu':\n",
    "        # See example: https://github.com/dmlc/xgboost/blob/master/demo/dask/gpu_training.py\n",
    "        # `DaskDeviceQuantileDMatrix` is used instead of `DaskDMatrix`, be careful\n",
    "        # that it can not be used for anything else than training.\n",
    "        dtrain = xgb.dask.DaskDeviceQuantileDMatrix(client, X, y, weight=weight)\n",
    "    else:\n",
    "        dtrain = DaskDMatrix(client, X, y, weight=weight)\n",
    "    \n",
    "#     del ddf_enc, X, y # Cleanup\n",
    "    data_load_time = np.round(time() - tic, 2)\n",
    "\n",
    "    tic = time()\n",
    "    xgb_model = xgb.dask.train(client, xgb_params, dtrain, num_boost_round=num_boost_round)['booster']\n",
    "    train_model_time = np.round(time() - tic, 2)\n",
    "\n",
    "    # Save xgb model to disk:\n",
    "    tic = time()\n",
    "    xgb_model.save_model(xgb_model_name+'.model') # Save model to current dir\n",
    "    xgb_model.save_model('/tmp/'+xgb_model_name+'.json') # Experimental JSON format. To be nested in output file with timings/metrics.\n",
    "\n",
    "    # Move local xgb-model to hdfs if xgb_hdfs_output_dir specified:\n",
    "    if (storage_backend == 'hdfs'):\n",
    "        if xgb_hdfs_output_dir != None:\n",
    "            with open(xgb_model_name+'.model','rb') as f:\n",
    "                hdfs.upload(os.path.join(xgb_hdfs_output_dir, xgb_model_name+'.model'), f)\n",
    "    export_model_time = np.round(time() - tic, 2)\n",
    "\n",
    "    metrics = {\n",
    "        'data_load_time': data_load_time,\n",
    "        'train_model_time': train_model_time,\n",
    "        'export_model_time': export_model_time\n",
    "        }\n",
    "\n",
    "    del dtrain\n",
    "    gc.collect() # Manual garbage collection needed since python may not clean up GPU resources automatically.\n",
    "    return(metrics)\n",
    "\n",
    "\n",
    "# xgboost params:\n",
    "xgb_params = { \n",
    "    'random_state': 0,\n",
    "    'eta': 0.1,\n",
    "    'gamma': 0.1,\n",
    "    'max_depth': max_depth, \n",
    "#     'num_boost_round': num_boost_round, # Number of boosting rounds: num_boost_round in xgboost, numRound in xgboost4j, n_estimators in scikit-learn.\n",
    "    'max_leaves': 4*256, # Maximum number of nodes to be added. Only relevant when grow_policy=lossguide is set.\n",
    "    'objective': xgb_objective,\n",
    "    'eval_metric': 'auc', # Applied to eval_set/test_data if provided\n",
    "    'booster': 'gbtree',\n",
    "}\n",
    "\n",
    "if run_type == 'cpu':\n",
    "    tree_method = 'hist'\n",
    "elif run_type == 'gpu':   \n",
    "    tree_method = 'gpu_hist'\n",
    "\n",
    "xgb_params['tree_method'] = tree_method\n",
    "\n",
    "\n",
    "y_tmp = dd.read_parquet('/dbfs/FileStore/tables/processed_data_june.parquet', columns=label_col)\n",
    "num_class = y_tmp.max().compute() + 1\n",
    "\n",
    "print('Number of classes detected: ', num_class)\n",
    "xgb_params['num_class'] = num_class\n",
    "\n",
    "print('Starting xgboost '+run_type+' '+pred_model+' training....')\n",
    "\n",
    "tic = time()\n",
    "\n",
    "# Large amount of memory required to train on full 16 yrs of data. Reduce number of years used during training on single node tests.\n",
    "training_metrics = dask_xgb_train_from_enc(client, run_type, train_dataset, label_col, xgb_params, xgb_model_name)\n",
    "\n",
    "t_dask_train = np.round(time() - tic, 2)\n",
    "\n",
    "training_metrics['total_time'] = t_dask_train\n",
    "print(training_metrics)\n",
    "print('Dask xgboost '+run_type+' training time:', '{:0.2f}'.format(t_dask_train) + 's')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "15600b4f-ec69-4a6e-b1db-df26a9075e62",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "keys = ['TAGS', 'ACTION', 'OBJECT','PRIOR_ACTIONS', 'ROOT CAUSE'] #'PLATFORM', 'CAPABILITIES', 'RESOLUTION']\n",
    "\n",
    "def train_tree_model(\n",
    "    train_data: pd.DataFrame, cv_params: dict, n_cv_folds: int, n_iter: int\n",
    "):\n",
    "    \"\"\"\n",
    "    :param train_data: Preprocessed dataframe that contains both the question and category\n",
    "    \"\"\"\n",
    "    columns =['TAGS', 'ACTION', 'OBJECT', 'PRIOR_ACTIONS', 'ROOT CAUSE']\n",
    "   # tfidf = TfidfVectorzer()\n",
    "   # for feature in columns:\n",
    "    #    df[feature + '_tfidf'] = tfidf.fit_transform(train_data[feature]).toarray()\n",
    "\n",
    "    #X = pd.concat([train_data[feature + '_tfidf'] for feature in train_data.columns], axis=1)\n",
    "    X = train_data[keys]\n",
    "    \n",
    "    le = LabelEncoder()\n",
    "    Y = le.fit_transform(train_data[\"CAT\"])\n",
    "    #Y = train_data[\"CAT\"]\n",
    "\n",
    "   # feature_union = FeatureUnion([\n",
    "   #     ('tfidf1', T)\n",
    "   # ])\n",
    "\n",
    "    classifier = Pipeline(\n",
    "        steps=[\n",
    "            ('vectorizer', TfidfVectorizer()),\n",
    "            (\"classifier\", XGBClassifier(random_state=10)),\n",
    "        ],\n",
    "        verbose=True,\n",
    "    )\n",
    "    classifier_random = RandomizedSearchCV(\n",
    "        classifier,\n",
    "        param_distributions=cv_params,\n",
    "        n_iter=n_iter,\n",
    "        verbose=0,\n",
    "        scoring=\"accuracy\",\n",
    "        random_state=42,\n",
    "        refit=True,\n",
    "        n_jobs=-1,\n",
    "        cv=StratifiedKFold(n_splits=n_cv_folds, shuffle=True, random_state=42),\n",
    "    )\n",
    "    classifier_random.fit(X, Y)\n",
    "    return classifier_random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d9298d29-914d-4bdd-a41d-14acb2f330dd",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Model storage function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a8d759fd-4f40-416c-88b2-4bceaa15e637",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "class SKLearnEstimatorWrapper(mlflow.pyfunc.PythonModel):\n",
    "    def __init__(self, estimator, mapping_dict):\n",
    "        self.estimator = estimator\n",
    "        self.mapping_dict = mapping_dict\n",
    "\n",
    "    def predict(self, context, model_input):\n",
    "        #image_features = np.array([hog(np.array(Image.open(io.BytesIO(model_input.loc[i, \"screenshot\"]))), channel_axis=-1) for i in range(len(model_input))]) \n",
    "\n",
    "        model_input = model_input[keys]#, 'image': image_features}\n",
    "\n",
    "        outputs = self.estimator.predict(model_input)\n",
    "        ret_d = {\"class\": np.array([self.mapping_dict[str(x)] for x in outputs])}\n",
    "        return ret_d\n",
    "\n",
    "\n",
    "def log_estimator_model(estimator, encode_dict):\n",
    "    \"\"\"\n",
    "    :param estimator: An estimator that takes as input text and outputs a category (for the question)\n",
    "    :param encode_dict: A dictionary which encodes a labelled category (such as 'Guest Rooms') to a numeric category\n",
    "    \"\"\"\n",
    "    mapping_dict = {str(v): k for k, v in encode_dict.items()}\n",
    "    wrapped_estimator = SKLearnEstimatorWrapper(estimator, mapping_dict)\n",
    "    input_example = {\n",
    "        \"text\": np.array(\n",
    "            test_dataset.loc[0, keys]\n",
    "        )\n",
    "        #'image': hog(np.array(Image.open(io.BytesIO(test_dataset.loc[0, \"screenshot\"]))), channel_axis=-1)\n",
    "    }\n",
    "    signature = infer_signature(\n",
    "        pd.DataFrame(input_example),\n",
    "        wrapped_estimator.predict(None, pd.DataFrame(input_example)),\n",
    "    )\n",
    "    # Specify the additional dependencies\n",
    "    conda_env = _mlflow_conda_env(\n",
    "        additional_conda_deps=None,\n",
    "        additional_pip_deps=[\n",
    "            \"xgboost=={}\".format(xgboost.__version__),\n",
    "            \"sklearn=={}\".format(sklearn.__version__),\n",
    "        ],\n",
    "        additional_conda_channels=None,\n",
    "    )\n",
    "    # Log model so the model artifacts can have the correct conda env, python model, and signature (data formats for input & output)\n",
    "    return mlflow.pyfunc.log_model(\n",
    "        \"Bluecumber_test_analysis\",\n",
    "        python_model=wrapped_estimator,\n",
    "        conda_env=conda_env,\n",
    "        signature=signature,\n",
    "        input_example=input_example,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "aaa531a0-100d-40fe-8884-0a69601b8662",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Perform CV with MlFlow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d5018f0b-37a8-4e74-98ff-d9ff8e1b5a73",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "client = MlflowClient()\n",
    "n_cv_folds = 3\n",
    "\n",
    "\n",
    "def init_experiment(experiment_name):\n",
    "    # fetch existing experiments, if it doesn't exist create it and set it\n",
    "    existing_experiments = client.search_experiments()\n",
    "    if experiment_name in [exp.name for exp in existing_experiments]:\n",
    "        return mlflow.set_experiment(experiment_name)\n",
    "    else:\n",
    "        return mlflow.create_experiment(experiment_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "86a059bc-aa48-48f9-84a3-ddef0188c13e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "com.databricks.rpc.RPCResponseTooLarge: rpc response (of 20985024 bytes) exceeds limit of 20971520 bytes\n",
       "\tat com.databricks.rpc.Jetty9Client$$anon$1.onContent(Jetty9Client.scala:651)\n",
       "\tat shaded.v9_4.org.eclipse.jetty.client.api.Response$Listener.onContent(Response.java:294)\n",
       "\tat shaded.v9_4.org.eclipse.jetty.client.api.Response$Listener.onContent(Response.java:306)\n",
       "\tat shaded.v9_4.org.eclipse.jetty.client.ResponseNotifier.notifyContent(ResponseNotifier.java:155)\n",
       "\tat shaded.v9_4.org.eclipse.jetty.client.ResponseNotifier.notifyContent(ResponseNotifier.java:139)\n",
       "\tat shaded.v9_4.org.eclipse.jetty.client.HttpReceiver$ContentListeners.notifyContent(HttpReceiver.java:725)\n",
       "\tat shaded.v9_4.org.eclipse.jetty.client.HttpReceiver$ContentListeners.access$500(HttpReceiver.java:688)\n",
       "\tat shaded.v9_4.org.eclipse.jetty.client.HttpReceiver.responseContent(HttpReceiver.java:409)\n",
       "\tat shaded.v9_4.org.eclipse.jetty.client.http.HttpReceiverOverHTTP.content(HttpReceiverOverHTTP.java:295)\n",
       "\tat shaded.v9_4.org.eclipse.jetty.http.HttpParser.parseContent(HttpParser.java:1815)\n",
       "\tat shaded.v9_4.org.eclipse.jetty.http.HttpParser.parseNext(HttpParser.java:1526)\n",
       "\tat shaded.v9_4.org.eclipse.jetty.client.http.HttpReceiverOverHTTP.parse(HttpReceiverOverHTTP.java:200)\n",
       "\tat shaded.v9_4.org.eclipse.jetty.client.http.HttpReceiverOverHTTP.process(HttpReceiverOverHTTP.java:141)\n",
       "\tat shaded.v9_4.org.eclipse.jetty.client.http.HttpReceiverOverHTTP.receive(HttpReceiverOverHTTP.java:75)\n",
       "\tat shaded.v9_4.org.eclipse.jetty.client.http.HttpChannelOverHTTP.receive(HttpChannelOverHTTP.java:133)\n",
       "\tat shaded.v9_4.org.eclipse.jetty.client.http.HttpConnectionOverHTTP.onFillable(HttpConnectionOverHTTP.java:151)\n",
       "\tat shaded.v9_4.org.eclipse.jetty.io.AbstractConnection$ReadCallback.succeeded(AbstractConnection.java:311)\n",
       "\tat shaded.v9_4.org.eclipse.jetty.io.FillInterest.fillable(FillInterest.java:103)\n",
       "\tat shaded.v9_4.org.eclipse.jetty.io.ssl.SslConnection$DecryptedEndPoint.onFillable(SslConnection.java:543)\n",
       "\tat shaded.v9_4.org.eclipse.jetty.io.ssl.SslConnection.onFillable(SslConnection.java:398)\n",
       "\tat shaded.v9_4.org.eclipse.jetty.io.ssl.SslConnection$2.succeeded(SslConnection.java:161)\n",
       "\tat shaded.v9_4.org.eclipse.jetty.io.FillInterest.fillable(FillInterest.java:103)\n",
       "\tat shaded.v9_4.org.eclipse.jetty.io.ChannelEndPoint$2.run(ChannelEndPoint.java:117)\n",
       "\tat shaded.v9_4.org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.runTask(EatWhatYouKill.java:336)\n",
       "\tat shaded.v9_4.org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.doProduce(EatWhatYouKill.java:313)\n",
       "\tat shaded.v9_4.org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.tryProduce(EatWhatYouKill.java:171)\n",
       "\tat shaded.v9_4.org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.run(EatWhatYouKill.java:129)\n",
       "\tat shaded.v9_4.org.eclipse.jetty.util.thread.ReservedThreadExecutor$ReservedThread.run(ReservedThreadExecutor.java:388)\n",
       "\tat com.databricks.rpc.ShadedInstrumentedQueuedThreadPool$$anon$2.$anonfun$run$6(InstrumentedQueuedThreadPool.scala:136)\n",
       "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
       "\tat com.databricks.instrumentation.QueuedThreadPoolInstrumenter.trackActiveThreads(QueuedThreadPoolInstrumenter.scala:66)\n",
       "\tat com.databricks.instrumentation.QueuedThreadPoolInstrumenter.trackActiveThreads$(QueuedThreadPoolInstrumenter.scala:63)\n",
       "\tat com.databricks.rpc.ShadedInstrumentedQueuedThreadPool.trackActiveThreads(InstrumentedQueuedThreadPool.scala:102)\n",
       "\tat com.databricks.rpc.ShadedInstrumentedQueuedThreadPool$$anon$2.run(InstrumentedQueuedThreadPool.scala:131)\n",
       "\tat shaded.v9_4.org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:806)\n",
       "\tat shaded.v9_4.org.eclipse.jetty.util.thread.QueuedThreadPool$Runner.run(QueuedThreadPool.java:938)\n",
       "\tat java.base/java.lang.Thread.run(Thread.java:829)"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "com.databricks.rpc.RPCResponseTooLarge: rpc response (of 20985024 bytes) exceeds limit of 20971520 bytes\n\tat com.databricks.rpc.Jetty9Client$$anon$1.onContent(Jetty9Client.scala:651)\n\tat shaded.v9_4.org.eclipse.jetty.client.api.Response$Listener.onContent(Response.java:294)\n\tat shaded.v9_4.org.eclipse.jetty.client.api.Response$Listener.onContent(Response.java:306)\n\tat shaded.v9_4.org.eclipse.jetty.client.ResponseNotifier.notifyContent(ResponseNotifier.java:155)\n\tat shaded.v9_4.org.eclipse.jetty.client.ResponseNotifier.notifyContent(ResponseNotifier.java:139)\n\tat shaded.v9_4.org.eclipse.jetty.client.HttpReceiver$ContentListeners.notifyContent(HttpReceiver.java:725)\n\tat shaded.v9_4.org.eclipse.jetty.client.HttpReceiver$ContentListeners.access$500(HttpReceiver.java:688)\n\tat shaded.v9_4.org.eclipse.jetty.client.HttpReceiver.responseContent(HttpReceiver.java:409)\n\tat shaded.v9_4.org.eclipse.jetty.client.http.HttpReceiverOverHTTP.content(HttpReceiverOverHTTP.java:295)\n\tat shaded.v9_4.org.eclipse.jetty.http.HttpParser.parseContent(HttpParser.java:1815)\n\tat shaded.v9_4.org.eclipse.jetty.http.HttpParser.parseNext(HttpParser.java:1526)\n\tat shaded.v9_4.org.eclipse.jetty.client.http.HttpReceiverOverHTTP.parse(HttpReceiverOverHTTP.java:200)\n\tat shaded.v9_4.org.eclipse.jetty.client.http.HttpReceiverOverHTTP.process(HttpReceiverOverHTTP.java:141)\n\tat shaded.v9_4.org.eclipse.jetty.client.http.HttpReceiverOverHTTP.receive(HttpReceiverOverHTTP.java:75)\n\tat shaded.v9_4.org.eclipse.jetty.client.http.HttpChannelOverHTTP.receive(HttpChannelOverHTTP.java:133)\n\tat shaded.v9_4.org.eclipse.jetty.client.http.HttpConnectionOverHTTP.onFillable(HttpConnectionOverHTTP.java:151)\n\tat shaded.v9_4.org.eclipse.jetty.io.AbstractConnection$ReadCallback.succeeded(AbstractConnection.java:311)\n\tat shaded.v9_4.org.eclipse.jetty.io.FillInterest.fillable(FillInterest.java:103)\n\tat shaded.v9_4.org.eclipse.jetty.io.ssl.SslConnection$DecryptedEndPoint.onFillable(SslConnection.java:543)\n\tat shaded.v9_4.org.eclipse.jetty.io.ssl.SslConnection.onFillable(SslConnection.java:398)\n\tat shaded.v9_4.org.eclipse.jetty.io.ssl.SslConnection$2.succeeded(SslConnection.java:161)\n\tat shaded.v9_4.org.eclipse.jetty.io.FillInterest.fillable(FillInterest.java:103)\n\tat shaded.v9_4.org.eclipse.jetty.io.ChannelEndPoint$2.run(ChannelEndPoint.java:117)\n\tat shaded.v9_4.org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.runTask(EatWhatYouKill.java:336)\n\tat shaded.v9_4.org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.doProduce(EatWhatYouKill.java:313)\n\tat shaded.v9_4.org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.tryProduce(EatWhatYouKill.java:171)\n\tat shaded.v9_4.org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.run(EatWhatYouKill.java:129)\n\tat shaded.v9_4.org.eclipse.jetty.util.thread.ReservedThreadExecutor$ReservedThread.run(ReservedThreadExecutor.java:388)\n\tat com.databricks.rpc.ShadedInstrumentedQueuedThreadPool$$anon$2.$anonfun$run$6(InstrumentedQueuedThreadPool.scala:136)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat com.databricks.instrumentation.QueuedThreadPoolInstrumenter.trackActiveThreads(QueuedThreadPoolInstrumenter.scala:66)\n\tat com.databricks.instrumentation.QueuedThreadPoolInstrumenter.trackActiveThreads$(QueuedThreadPoolInstrumenter.scala:63)\n\tat com.databricks.rpc.ShadedInstrumentedQueuedThreadPool.trackActiveThreads(InstrumentedQueuedThreadPool.scala:102)\n\tat com.databricks.rpc.ShadedInstrumentedQueuedThreadPool$$anon$2.run(InstrumentedQueuedThreadPool.scala:131)\n\tat shaded.v9_4.org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:806)\n\tat shaded.v9_4.org.eclipse.jetty.util.thread.QueuedThreadPool$Runner.run(QueuedThreadPool.java:938)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
       "errorSummary": "Internal error. Attach your notebook to a different compute or restart the current compute.",
       "errorTraceType": null,
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "keys = ['TAGS', 'ACTION', 'OBJECT', 'PRIOR_ACTIONS', 'ROOT CAUSE'] #'PLATFORM', 'CAPABILITIES', 'RESOLUTION']\n",
    "\n",
    "\n",
    "mlflow.end_run()\n",
    "#experiment_id = init_experiment(\n",
    "  #  \"/Users/Chris.Jose@cvent.com/test_result_analysis/experiments/#xgboost_test_result_analysis_1\")\n",
    "#run = mlflow.start_run(run_name=\"xgboost_tfidf_CV\", #experiment_id=experiment_id)\n",
    "\n",
    "experiment_name = \"/Users/Chris.Jose@cvent.com/test_result_analysis/experiments/#xgboost_test_result_analysis_2\"\n",
    "mlflow.set_experiment(experiment_name)  \n",
    "  \n",
    "# Get the experiment ID  \n",
    "experiment = mlflow.get_experiment_by_name(experiment_name)  \n",
    "experiment_id = experiment.experiment_id  \n",
    "  \n",
    "# Start a new MLflow run  \n",
    "run = mlflow.start_run(run_name=\"xgboost_tfidf_CV\", experiment_id=experiment_id)\n",
    "\n",
    "\n",
    "mlflow.sklearn.autolog(log_models=False)\n",
    "mlflow.log_param(\"n_cv_folds\", 3)\n",
    "classifier_random = train_tree_model(\n",
    "    train_dataset, get_params(), n_cv_folds, n_iter=30000\n",
    ")\n",
    "print(classifier_random.best_score_)\n",
    "#test_image_features = np.array([hog(np.array(Image.open(io.BytesIO(test_dataset.loc[i, \"screenshot\"]))), channel_axis=-1) for i in range(len(test_dataset))]) \n",
    "\n",
    "#model_input = {'text': test_dataset[\"StepDescription\"], 'image': test_image_features}\n",
    "X_test = test_dataset[keys]\n",
    "Y_test = test_dataset[\"CAT\"]\n",
    "mlflow.log_metric(\n",
    "    \"testing_accuracy_score\", classifier_random.best_estimator_.score(X_test, Y_test)\n",
    ")\n",
    "estimator = classifier_random.best_estimator_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "de634849-ab60-485c-b39a-0c7fc32c4431",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Test set metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0f6b6d60-f3e0-47a8-9be3-653273a42ad5",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "y_pred = classifier_random.best_estimator_.predict(X_test)\n",
    "y_true = Y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ff723f5f-b41f-4292-9c6c-4f70d8ee4ef5",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Confusion matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "94d12a90-a5ba-4cbd-9eb9-d5504bd6baba",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "font = {\"family\": \"normal\", \"size\": 15}\n",
    "\n",
    "matplotlib.rc(\"font\", **font)\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "fig, ax = plt.subplots()\n",
    "sns.heatmap(\n",
    "    cm,\n",
    "    annot=True,\n",
    "    annot_kws={\"size\": 12},\n",
    "    ax=ax,\n",
    "    cmap=\"Blues\",\n",
    "    fmt=\"g\",\n",
    "    xticklabels=list(encode_dict.keys()),\n",
    "    yticklabels=list(encode_dict.keys()),\n",
    ")\n",
    "plt.xticks(rotation=45, horizontalalignment=\"right\")\n",
    "fig.set_size_inches(12, 12)\n",
    "plt.title(\"Confusion Matrix - Test Set\", fontdict={\"family\": \"normal\", \"size\": 21})\n",
    "img_buf = io.BytesIO()\n",
    "plt.ylabel(\"True label\", fontdict={\"family\": \"normal\", \"size\": 21})\n",
    "plt.xlabel(\"Predicted label\", fontdict={\"family\": \"normal\", \"size\": 21})\n",
    "plt.show()\n",
    "fig.savefig(img_buf, dpi=fig.dpi, bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4ff9e879-d0a8-4f22-8ef0-a047a298d0b2",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "font = {\"family\": \"normal\", \"size\": 15}\n",
    "\n",
    "matplotlib.rc(\"font\", **font)\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "fig, ax = plt.subplots()\n",
    "sns.heatmap(\n",
    "    cm / cm.sum(axis=1).reshape(-1, 1),\n",
    "    annot=True,\n",
    "    annot_kws={\"size\": 12},\n",
    "    ax=ax,\n",
    "    cmap=\"Blues\",\n",
    "    fmt=\".2%\",\n",
    "    xticklabels=list(encode_dict.keys()),\n",
    "    yticklabels=list(encode_dict.keys()),\n",
    ")\n",
    "plt.xticks(rotation=45, horizontalalignment=\"right\")\n",
    "fig.set_size_inches(12, 12)\n",
    "plt.title(\n",
    "    \"Confusion Matrix - Test Set Percentage\", fontdict={\"family\": \"normal\", \"size\": 21}\n",
    ")\n",
    "plt.ylabel(\"True label\", fontdict={\"family\": \"normal\", \"size\": 21})\n",
    "plt.xlabel(\"Predicted label\", fontdict={\"family\": \"normal\", \"size\": 21})\n",
    "img_buf2 = io.BytesIO()\n",
    "plt.show()\n",
    "fig.savefig(img_buf2, dpi=fig.dpi, bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4a7bddbf-12ac-45b3-ba1c-7b48751d70ef",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## F1, precision, recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b8739e6c-6d52-49f8-aba5-9a5bb80c218b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "how = [\"micro\", \"macro\", \"weighted\"]\n",
    "metrics = {k: {\"f1\": 0, \"precision\": 0, \"recall\": 0} for k in how}\n",
    "for k in how:\n",
    "    metrics[k][\"f1\"] = f1_score(y_true, y_pred, average=k)\n",
    "    metrics[k][\"precision\"] = precision_score(y_true, y_pred, average=k)\n",
    "    metrics[k][\"recall\"] = recall_score(y_true, y_pred, average=k)\n",
    "# now print it nicely\n",
    "print(\"Metrics for test set!\")\n",
    "print(\"+\" * 29)\n",
    "for k in how:\n",
    "    print(f\"{k.upper()} average:\")\n",
    "    for k2 in metrics[k]:\n",
    "        print(f\"\\t{k2}: {metrics[k][k2]}\")\n",
    "    print(\"-\" * 29)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d5013e3c-a9aa-4d0f-9457-7275c6d41491",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Log confusion matrix + metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "05e2af72-520d-474f-97e4-7c6c62cea90f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "im = Image.open(img_buf)\n",
    "mlflow.log_image(im, \"confusion_matrix.png\")\n",
    "img_buf.close()\n",
    "# Now do percentage matrix\n",
    "im = Image.open(img_buf2)\n",
    "mlflow.log_image(im, \"confusion_matrix_percentage.png\")\n",
    "img_buf.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2e76ae50-96b4-4a1e-b2e7-c1de335ca08d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# log f1,prec,recall for micro, macro, and weighted (so 9 total)\n",
    "for k in how:\n",
    "    for k2 in metrics[k]:\n",
    "        mlflow.log_metric(f\"{k}_{k2}\", metrics[k][k2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8d5bb8df-d1b9-4b64-aa7a-6a3ec5b7ea81",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Log model to run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "12d04459-d830-4c3e-b0c4-244d7c44d43e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "log_estimator_model(estimator, encode_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "00af6495-447b-46b5-9bd6-4a58b3927561",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# End run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "325874bb-7fb3-4a06-b858-b17d540efb57",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "mlflow.end_run()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "xgboost-dask-classifier",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
