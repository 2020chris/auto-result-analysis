{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8bcf8724-b2fe-45de-81c5-c8a65e620fc1",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[43mNote: you may need to restart the kernel using dbutils.library.restartPython() to use updated packages.\u001B[0m\nRequirement already satisfied: scikit-image in /local_disk0/.ephemeral_nfs/envs/pythonEnv-5c39f2a7-fb12-4c6c-a143-384f7bb10086/lib/python3.10/site-packages (0.21.0)\nRequirement already satisfied: networkx>=2.8 in /databricks/python3/lib/python3.10/site-packages (from scikit-image) (2.8.4)\nRequirement already satisfied: pillow>=9.0.1 in /databricks/python3/lib/python3.10/site-packages (from scikit-image) (9.2.0)\nRequirement already satisfied: scipy>=1.8 in /databricks/python3/lib/python3.10/site-packages (from scikit-image) (1.9.1)\nRequirement already satisfied: tifffile>=2022.8.12 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-5c39f2a7-fb12-4c6c-a143-384f7bb10086/lib/python3.10/site-packages (from scikit-image) (2023.7.18)\nRequirement already satisfied: imageio>=2.27 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-5c39f2a7-fb12-4c6c-a143-384f7bb10086/lib/python3.10/site-packages (from scikit-image) (2.31.1)\nRequirement already satisfied: PyWavelets>=1.1.1 in /databricks/python3/lib/python3.10/site-packages (from scikit-image) (1.3.0)\nRequirement already satisfied: numpy>=1.21.1 in /databricks/python3/lib/python3.10/site-packages (from scikit-image) (1.21.5)\nRequirement already satisfied: lazy_loader>=0.2 in /databricks/python3/lib/python3.10/site-packages (from scikit-image) (0.3)\nRequirement already satisfied: packaging>=21 in /databricks/python3/lib/python3.10/site-packages (from scikit-image) (21.3)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /databricks/python3/lib/python3.10/site-packages (from packaging>=21->scikit-image) (3.0.9)\n\u001B[43mNote: you may need to restart the kernel using dbutils.library.restartPython() to use updated packages.\u001B[0m\n\u001B[43mNote: you may need to restart the kernel using dbutils.library.restartPython() to use updated packages.\u001B[0m\nLooking in indexes: https://pypi.org/simple, https://pypi.nvidia.com\nCollecting cudf-cu11\n  Using cached https://pypi.nvidia.com/cudf-cu11/cudf_cu11-23.8.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (499.6 MB)\nRequirement already satisfied: fsspec>=0.6.0 in /databricks/python3/lib/python3.10/site-packages (from cudf-cu11) (2022.7.1)\nCollecting nvtx>=0.2.1\n  Using cached nvtx-0.2.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (553 kB)\nRequirement already satisfied: packaging in /databricks/python3/lib/python3.10/site-packages (from cudf-cu11) (21.3)\nRequirement already satisfied: cachetools in /databricks/python3/lib/python3.10/site-packages (from cudf-cu11) (4.2.4)\nCollecting cupy-cuda11x>=12.0.0\n  Using cached cupy_cuda11x-12.2.0-cp310-cp310-manylinux2014_x86_64.whl (89.6 MB)\nRequirement already satisfied: numpy>=1.21 in /databricks/python3/lib/python3.10/site-packages (from cudf-cu11) (1.21.5)\nRequirement already satisfied: typing-extensions>=4.0.0 in /databricks/python3/lib/python3.10/site-packages (from cudf-cu11) (4.3.0)\nCollecting numba>=0.57\n  Using cached numba-0.57.1-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (3.6 MB)\nRequirement already satisfied: pandas<1.6.0dev0,>=1.3 in /databricks/python3/lib/python3.10/site-packages (from cudf-cu11) (1.4.4)\nCollecting cuda-python<12.0a0,>=11.7.1\n  Using cached cuda_python-11.8.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (16.5 MB)\nCollecting ptxcompiler-cu11\n  Using cached https://pypi.nvidia.com/ptxcompiler-cu11/ptxcompiler_cu11-0.7.0.post1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (8.8 MB)\nCollecting pyarrow==11.*\n  Using cached pyarrow-11.0.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (34.9 MB)\nCollecting cubinlinker-cu11\n  Using cached https://pypi.nvidia.com/cubinlinker-cu11/cubinlinker_cu11-0.3.0.post1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (8.8 MB)\nCollecting protobuf<5,>=4.21\n  Using cached protobuf-4.24.0-cp37-abi3-manylinux2014_x86_64.whl (311 kB)\nCollecting rmm-cu11==23.8.*\n  Using cached https://pypi.nvidia.com/rmm-cu11/rmm_cu11-23.8.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.7 MB)\nRequirement already satisfied: cython in /databricks/python3/lib/python3.10/site-packages (from cuda-python<12.0a0,>=11.7.1->cudf-cu11) (0.29.32)\nCollecting fastrlock>=0.5\n  Using cached fastrlock-0.8.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_24_x86_64.whl (47 kB)\nCollecting llvmlite<0.41,>=0.40.0dev0\n  Using cached llvmlite-0.40.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (42.1 MB)\nRequirement already satisfied: pytz>=2020.1 in /databricks/python3/lib/python3.10/site-packages (from pandas<1.6.0dev0,>=1.3->cudf-cu11) (2022.1)\nRequirement already satisfied: python-dateutil>=2.8.1 in /databricks/python3/lib/python3.10/site-packages (from pandas<1.6.0dev0,>=1.3->cudf-cu11) (2.8.2)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /databricks/python3/lib/python3.10/site-packages (from packaging->cudf-cu11) (3.0.9)\nRequirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil>=2.8.1->pandas<1.6.0dev0,>=1.3->cudf-cu11) (1.16.0)\nInstalling collected packages: ptxcompiler-cu11, nvtx, fastrlock, cubinlinker-cu11, pyarrow, protobuf, llvmlite, cupy-cuda11x, cuda-python, numba, rmm-cu11, cudf-cu11\n  Attempting uninstall: pyarrow\n    Found existing installation: pyarrow 8.0.0\n    Not uninstalling pyarrow at /databricks/python3/lib/python3.10/site-packages, outside environment /local_disk0/.ephemeral_nfs/envs/pythonEnv-5c39f2a7-fb12-4c6c-a143-384f7bb10086\n    Can't uninstall 'pyarrow'. No files were found to uninstall.\n  Attempting uninstall: protobuf\n    Found existing installation: protobuf 3.19.4\n    Not uninstalling protobuf at /databricks/python3/lib/python3.10/site-packages, outside environment /local_disk0/.ephemeral_nfs/envs/pythonEnv-5c39f2a7-fb12-4c6c-a143-384f7bb10086\n    Can't uninstall 'protobuf'. No files were found to uninstall.\n  Attempting uninstall: llvmlite\n    Found existing installation: llvmlite 0.38.0\n    Not uninstalling llvmlite at /databricks/python3/lib/python3.10/site-packages, outside environment /local_disk0/.ephemeral_nfs/envs/pythonEnv-5c39f2a7-fb12-4c6c-a143-384f7bb10086\n    Can't uninstall 'llvmlite'. No files were found to uninstall.\n  Attempting uninstall: numba\n    Found existing installation: numba 0.55.1\n    Not uninstalling numba at /databricks/python3/lib/python3.10/site-packages, outside environment /local_disk0/.ephemeral_nfs/envs/pythonEnv-5c39f2a7-fb12-4c6c-a143-384f7bb10086\n    Can't uninstall 'numba'. No files were found to uninstall.\nERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\npetastorm 0.12.1 requires pyspark>=2.1.0, which is not installed.\ndatabricks-feature-store 0.14.0 requires pyspark<4,>=3.1.2, which is not installed.\ntensorflow 2.11.1 requires protobuf<3.20,>=3.9.2, but you have protobuf 4.24.0 which is incompatible.\ntensorboard 2.11.0 requires protobuf<4,>=3.9.2, but you have protobuf 4.24.0 which is incompatible.\nSuccessfully installed cubinlinker-cu11-0.3.0.post1 cuda-python-11.8.2 cudf-cu11-23.8.0 cupy-cuda11x-12.2.0 fastrlock-0.8.1 llvmlite-0.40.1 numba-0.57.1 nvtx-0.2.6 protobuf-4.24.0 ptxcompiler-cu11-0.7.0.post1 pyarrow-11.0.0 rmm-cu11-23.8.0\n\u001B[43mNote: you may need to restart the kernel using dbutils.library.restartPython() to use updated packages.\u001B[0m\n\u001B[43mNote: you may need to restart the kernel using dbutils.library.restartPython() to use updated packages.\u001B[0m\nLooking in indexes: https://pypi.org/simple, https://pypi.nvidia.com\nCollecting cuml-cu11\n  Using cached https://pypi.nvidia.com/cuml-cu11/cuml_cu11-23.8.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1081.6 MB)\nRequirement already satisfied: numba>=0.57 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-5c39f2a7-fb12-4c6c-a143-384f7bb10086/lib/python3.10/site-packages (from cuml-cu11) (0.57.1)\nCollecting dask==2023.7.1\n  Using cached dask-2023.7.1-py3-none-any.whl (1.2 MB)\nRequirement already satisfied: cudf-cu11==23.8.* in /local_disk0/.ephemeral_nfs/envs/pythonEnv-5c39f2a7-fb12-4c6c-a143-384f7bb10086/lib/python3.10/site-packages (from cuml-cu11) (23.8.0)\nCollecting dask-cudf-cu11==23.8.*\n  Using cached https://pypi.nvidia.com/dask-cudf-cu11/dask_cudf_cu11-23.8.0-py3-none-any.whl (81 kB)\nCollecting treelite-runtime==3.2.0\n  Using cached treelite_runtime-3.2.0-py3-none-manylinux2014_x86_64.whl (198 kB)\nCollecting treelite==3.2.0\n  Using cached treelite-3.2.0-py3-none-manylinux2014_x86_64.whl (1.0 MB)\nRequirement already satisfied: cupy-cuda11x>=12.0.0 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-5c39f2a7-fb12-4c6c-a143-384f7bb10086/lib/python3.10/site-packages (from cuml-cu11) (12.2.0)\nCollecting distributed==2023.7.1\n  Using cached distributed-2023.7.1-py3-none-any.whl (985 kB)\nCollecting raft-dask-cu11==23.8.*\n  Using cached https://pypi.nvidia.com/raft-dask-cu11/raft_dask_cu11-23.8.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (214.7 MB)\nCollecting dask-cuda==23.8.*\n  Using cached dask_cuda-23.8.0-py3-none-any.whl (122 kB)\nRequirement already satisfied: scipy>=1.8.0 in /databricks/python3/lib/python3.10/site-packages (from cuml-cu11) (1.9.1)\nRequirement already satisfied: joblib>=0.11 in /databricks/python3/lib/python3.10/site-packages (from cuml-cu11) (1.2.0)\nRequirement already satisfied: packaging in /databricks/python3/lib/python3.10/site-packages (from cudf-cu11==23.8.*->cuml-cu11) (21.3)\nRequirement already satisfied: rmm-cu11==23.8.* in /local_disk0/.ephemeral_nfs/envs/pythonEnv-5c39f2a7-fb12-4c6c-a143-384f7bb10086/lib/python3.10/site-packages (from cudf-cu11==23.8.*->cuml-cu11) (23.8.0)\nRequirement already satisfied: pandas<1.6.0dev0,>=1.3 in /databricks/python3/lib/python3.10/site-packages (from cudf-cu11==23.8.*->cuml-cu11) (1.4.4)\nRequirement already satisfied: numpy>=1.21 in /databricks/python3/lib/python3.10/site-packages (from cudf-cu11==23.8.*->cuml-cu11) (1.21.5)\nRequirement already satisfied: fsspec>=0.6.0 in /databricks/python3/lib/python3.10/site-packages (from cudf-cu11==23.8.*->cuml-cu11) (2022.7.1)\nRequirement already satisfied: nvtx>=0.2.1 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-5c39f2a7-fb12-4c6c-a143-384f7bb10086/lib/python3.10/site-packages (from cudf-cu11==23.8.*->cuml-cu11) (0.2.6)\nRequirement already satisfied: cubinlinker-cu11 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-5c39f2a7-fb12-4c6c-a143-384f7bb10086/lib/python3.10/site-packages (from cudf-cu11==23.8.*->cuml-cu11) (0.3.0.post1)\nRequirement already satisfied: ptxcompiler-cu11 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-5c39f2a7-fb12-4c6c-a143-384f7bb10086/lib/python3.10/site-packages (from cudf-cu11==23.8.*->cuml-cu11) (0.7.0.post1)\nRequirement already satisfied: cachetools in /databricks/python3/lib/python3.10/site-packages (from cudf-cu11==23.8.*->cuml-cu11) (4.2.4)\nRequirement already satisfied: typing-extensions>=4.0.0 in /databricks/python3/lib/python3.10/site-packages (from cudf-cu11==23.8.*->cuml-cu11) (4.3.0)\nRequirement already satisfied: cuda-python<12.0a0,>=11.7.1 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-5c39f2a7-fb12-4c6c-a143-384f7bb10086/lib/python3.10/site-packages (from cudf-cu11==23.8.*->cuml-cu11) (11.8.2)\nRequirement already satisfied: protobuf<5,>=4.21 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-5c39f2a7-fb12-4c6c-a143-384f7bb10086/lib/python3.10/site-packages (from cudf-cu11==23.8.*->cuml-cu11) (4.24.0)\nRequirement already satisfied: pyarrow==11.* in /local_disk0/.ephemeral_nfs/envs/pythonEnv-5c39f2a7-fb12-4c6c-a143-384f7bb10086/lib/python3.10/site-packages (from cudf-cu11==23.8.*->cuml-cu11) (11.0.0)\nCollecting partd>=1.2.0\n  Using cached partd-1.4.0-py3-none-any.whl (18 kB)\nRequirement already satisfied: cloudpickle>=1.5.0 in /databricks/python3/lib/python3.10/site-packages (from dask==2023.7.1->cuml-cu11) (2.0.0)\nCollecting importlib-metadata>=4.13.0\n  Using cached importlib_metadata-6.8.0-py3-none-any.whl (22 kB)\nRequirement already satisfied: pyyaml>=5.3.1 in /databricks/python3/lib/python3.10/site-packages (from dask==2023.7.1->cuml-cu11) (6.0)\nCollecting toolz>=0.10.0\n  Using cached toolz-0.12.0-py3-none-any.whl (55 kB)\nRequirement already satisfied: click>=8.0 in /databricks/python3/lib/python3.10/site-packages (from dask==2023.7.1->cuml-cu11) (8.0.4)\nCollecting pynvml<11.5,>=11.0.0\n  Using cached pynvml-11.4.1-py3-none-any.whl (46 kB)\nCollecting zict>=2.0.0\n  Using cached zict-3.0.0-py2.py3-none-any.whl (43 kB)\nRequirement already satisfied: urllib3>=1.24.3 in /databricks/python3/lib/python3.10/site-packages (from distributed==2023.7.1->cuml-cu11) (1.26.11)\nRequirement already satisfied: msgpack>=1.0.0 in /databricks/python3/lib/python3.10/site-packages (from distributed==2023.7.1->cuml-cu11) (1.0.5)\nCollecting tblib>=1.6.0\n  Using cached tblib-2.0.0-py3-none-any.whl (11 kB)\nRequirement already satisfied: psutil>=5.7.2 in /databricks/python3/lib/python3.10/site-packages (from distributed==2023.7.1->cuml-cu11) (5.9.0)\nCollecting sortedcontainers>=2.0.5\n  Using cached sortedcontainers-2.4.0-py2.py3-none-any.whl (29 kB)\nRequirement already satisfied: jinja2>=2.10.3 in /databricks/python3/lib/python3.10/site-packages (from distributed==2023.7.1->cuml-cu11) (2.11.3)\nRequirement already satisfied: tornado>=6.0.4 in /databricks/python3/lib/python3.10/site-packages (from distributed==2023.7.1->cuml-cu11) (6.1)\nCollecting locket>=1.0.0\n  Using cached locket-1.0.0-py2.py3-none-any.whl (4.4 kB)\nCollecting ucx-py-cu11==0.33.*\n  Using cached https://pypi.nvidia.com/ucx-py-cu11/ucx_py_cu11-0.33.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.9 MB)\nCollecting pylibraft-cu11==23.8.*\n  Using cached https://pypi.nvidia.com/pylibraft-cu11/pylibraft_cu11-23.8.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (555.6 MB)\nRequirement already satisfied: fastrlock>=0.5 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-5c39f2a7-fb12-4c6c-a143-384f7bb10086/lib/python3.10/site-packages (from cupy-cuda11x>=12.0.0->cuml-cu11) (0.8.1)\nRequirement already satisfied: llvmlite<0.41,>=0.40.0dev0 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-5c39f2a7-fb12-4c6c-a143-384f7bb10086/lib/python3.10/site-packages (from numba>=0.57->cuml-cu11) (0.40.1)\nRequirement already satisfied: cython in /databricks/python3/lib/python3.10/site-packages (from cuda-python<12.0a0,>=11.7.1->cudf-cu11==23.8.*->cuml-cu11) (0.29.32)\nRequirement already satisfied: zipp>=0.5 in /databricks/python3/lib/python3.10/site-packages (from importlib-metadata>=4.13.0->dask==2023.7.1->cuml-cu11) (3.8.0)\nRequirement already satisfied: MarkupSafe>=0.23 in /databricks/python3/lib/python3.10/site-packages (from jinja2>=2.10.3->distributed==2023.7.1->cuml-cu11) (2.0.1)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /databricks/python3/lib/python3.10/site-packages (from packaging->cudf-cu11==23.8.*->cuml-cu11) (3.0.9)\nRequirement already satisfied: pytz>=2020.1 in /databricks/python3/lib/python3.10/site-packages (from pandas<1.6.0dev0,>=1.3->cudf-cu11==23.8.*->cuml-cu11) (2022.1)\nRequirement already satisfied: python-dateutil>=2.8.1 in /databricks/python3/lib/python3.10/site-packages (from pandas<1.6.0dev0,>=1.3->cudf-cu11==23.8.*->cuml-cu11) (2.8.2)\nRequirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil>=2.8.1->pandas<1.6.0dev0,>=1.3->cudf-cu11==23.8.*->cuml-cu11) (1.16.0)\nInstalling collected packages: sortedcontainers, zict, toolz, tblib, pynvml, locket, importlib-metadata, ucx-py-cu11, treelite-runtime, treelite, partd, pylibraft-cu11, dask, distributed, dask-cudf-cu11, dask-cuda, raft-dask-cu11, cuml-cu11\n  Attempting uninstall: importlib-metadata\n    Found existing installation: importlib-metadata 4.11.3\n    Not uninstalling importlib-metadata at /databricks/python3/lib/python3.10/site-packages, outside environment /local_disk0/.ephemeral_nfs/envs/pythonEnv-5c39f2a7-fb12-4c6c-a143-384f7bb10086\n    Can't uninstall 'importlib-metadata'. No files were found to uninstall.\nERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ndatabricks-feature-store 0.14.0 requires pyspark<4,>=3.1.2, which is not installed.\nSuccessfully installed cuml-cu11-23.8.0 dask-2023.7.1 dask-cuda-23.8.0 dask-cudf-cu11-23.8.0 distributed-2023.7.1 importlib-metadata-6.8.0 locket-1.0.0 partd-1.4.0 pylibraft-cu11-23.8.0 pynvml-11.4.1 raft-dask-cu11-23.8.0 sortedcontainers-2.4.0 tblib-2.0.0 toolz-0.12.0 treelite-3.2.0 treelite-runtime-3.2.0 ucx-py-cu11-0.33.0 zict-3.0.0\n\u001B[43mNote: you may need to restart the kernel using dbutils.library.restartPython() to use updated packages.\u001B[0m\n\u001B[43mNote: you may need to restart the kernel using dbutils.library.restartPython() to use updated packages.\u001B[0m\nCollecting dask-ml\n  Downloading dask_ml-2023.3.24-py3-none-any.whl (148 kB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 148.7/148.7 kB 3.5 MB/s eta 0:00:00\nRequirement already satisfied: scipy in /databricks/python3/lib/python3.10/site-packages (from dask-ml) (1.9.1)\nRequirement already satisfied: pandas>=0.24.2 in /databricks/python3/lib/python3.10/site-packages (from dask-ml) (1.4.4)\nRequirement already satisfied: numba>=0.51.0 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-5c39f2a7-fb12-4c6c-a143-384f7bb10086/lib/python3.10/site-packages (from dask-ml) (0.57.1)\nRequirement already satisfied: dask[array,dataframe]>=2.4.0 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-5c39f2a7-fb12-4c6c-a143-384f7bb10086/lib/python3.10/site-packages (from dask-ml) (2023.7.1)\nCollecting scikit-learn>=1.2.0\n  Downloading scikit_learn-1.3.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (10.8 MB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 10.8/10.8 MB 38.4 MB/s eta 0:00:00\nRequirement already satisfied: distributed>=2.4.0 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-5c39f2a7-fb12-4c6c-a143-384f7bb10086/lib/python3.10/site-packages (from dask-ml) (2023.7.1)\nRequirement already satisfied: numpy>=1.20.0 in /databricks/python3/lib/python3.10/site-packages (from dask-ml) (1.21.5)\nCollecting multipledispatch>=0.4.9\n  Downloading multipledispatch-1.0.0-py3-none-any.whl (12 kB)\nRequirement already satisfied: packaging in /databricks/python3/lib/python3.10/site-packages (from dask-ml) (21.3)\nCollecting dask-glm>=0.2.0\n  Downloading dask_glm-0.2.0-py2.py3-none-any.whl (12 kB)\nRequirement already satisfied: cloudpickle>=0.2.2 in /databricks/python3/lib/python3.10/site-packages (from dask-glm>=0.2.0->dask-ml) (2.0.0)\nRequirement already satisfied: pyyaml>=5.3.1 in /databricks/python3/lib/python3.10/site-packages (from dask[array,dataframe]>=2.4.0->dask-ml) (6.0)\nRequirement already satisfied: toolz>=0.10.0 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-5c39f2a7-fb12-4c6c-a143-384f7bb10086/lib/python3.10/site-packages (from dask[array,dataframe]>=2.4.0->dask-ml) (0.12.0)\nRequirement already satisfied: importlib-metadata>=4.13.0 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-5c39f2a7-fb12-4c6c-a143-384f7bb10086/lib/python3.10/site-packages (from dask[array,dataframe]>=2.4.0->dask-ml) (6.8.0)\nRequirement already satisfied: partd>=1.2.0 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-5c39f2a7-fb12-4c6c-a143-384f7bb10086/lib/python3.10/site-packages (from dask[array,dataframe]>=2.4.0->dask-ml) (1.4.0)\nRequirement already satisfied: fsspec>=2021.09.0 in /databricks/python3/lib/python3.10/site-packages (from dask[array,dataframe]>=2.4.0->dask-ml) (2022.7.1)\nRequirement already satisfied: click>=8.0 in /databricks/python3/lib/python3.10/site-packages (from dask[array,dataframe]>=2.4.0->dask-ml) (8.0.4)\nRequirement already satisfied: urllib3>=1.24.3 in /databricks/python3/lib/python3.10/site-packages (from distributed>=2.4.0->dask-ml) (1.26.11)\nRequirement already satisfied: psutil>=5.7.2 in /databricks/python3/lib/python3.10/site-packages (from distributed>=2.4.0->dask-ml) (5.9.0)\nRequirement already satisfied: zict>=2.2.0 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-5c39f2a7-fb12-4c6c-a143-384f7bb10086/lib/python3.10/site-packages (from distributed>=2.4.0->dask-ml) (3.0.0)\nRequirement already satisfied: jinja2>=2.10.3 in /databricks/python3/lib/python3.10/site-packages (from distributed>=2.4.0->dask-ml) (2.11.3)\nRequirement already satisfied: locket>=1.0.0 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-5c39f2a7-fb12-4c6c-a143-384f7bb10086/lib/python3.10/site-packages (from distributed>=2.4.0->dask-ml) (1.0.0)\nRequirement already satisfied: tblib>=1.6.0 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-5c39f2a7-fb12-4c6c-a143-384f7bb10086/lib/python3.10/site-packages (from distributed>=2.4.0->dask-ml) (2.0.0)\nRequirement already satisfied: msgpack>=1.0.0 in /databricks/python3/lib/python3.10/site-packages (from distributed>=2.4.0->dask-ml) (1.0.5)\nRequirement already satisfied: sortedcontainers>=2.0.5 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-5c39f2a7-fb12-4c6c-a143-384f7bb10086/lib/python3.10/site-packages (from distributed>=2.4.0->dask-ml) (2.4.0)\nRequirement already satisfied: tornado>=6.0.4 in /databricks/python3/lib/python3.10/site-packages (from distributed>=2.4.0->dask-ml) (6.1)\nRequirement already satisfied: llvmlite<0.41,>=0.40.0dev0 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-5c39f2a7-fb12-4c6c-a143-384f7bb10086/lib/python3.10/site-packages (from numba>=0.51.0->dask-ml) (0.40.1)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /databricks/python3/lib/python3.10/site-packages (from packaging->dask-ml) (3.0.9)\nRequirement already satisfied: pytz>=2020.1 in /databricks/python3/lib/python3.10/site-packages (from pandas>=0.24.2->dask-ml) (2022.1)\nRequirement already satisfied: python-dateutil>=2.8.1 in /databricks/python3/lib/python3.10/site-packages (from pandas>=0.24.2->dask-ml) (2.8.2)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /databricks/python3/lib/python3.10/site-packages (from scikit-learn>=1.2.0->dask-ml) (2.2.0)\nRequirement already satisfied: joblib>=1.1.1 in /databricks/python3/lib/python3.10/site-packages (from scikit-learn>=1.2.0->dask-ml) (1.2.0)\nRequirement already satisfied: zipp>=0.5 in /databricks/python3/lib/python3.10/site-packages (from importlib-metadata>=4.13.0->dask[array,dataframe]>=2.4.0->dask-ml) (3.8.0)\nRequirement already satisfied: MarkupSafe>=0.23 in /databricks/python3/lib/python3.10/site-packages (from jinja2>=2.10.3->distributed>=2.4.0->dask-ml) (2.0.1)\nRequirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil>=2.8.1->pandas>=0.24.2->dask-ml) (1.16.0)\nInstalling collected packages: multipledispatch, scikit-learn, dask-glm, dask-ml\n  Attempting uninstall: scikit-learn\n    Found existing installation: scikit-learn 1.1.1\n    Not uninstalling scikit-learn at /databricks/python3/lib/python3.10/site-packages, outside environment /local_disk0/.ephemeral_nfs/envs/pythonEnv-5c39f2a7-fb12-4c6c-a143-384f7bb10086\n    Can't uninstall 'scikit-learn'. No files were found to uninstall.\nERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nmleap 0.20.0 requires scikit-learn<0.23.0,>=0.22.0, but you have scikit-learn 1.3.0 which is incompatible.\nSuccessfully installed dask-glm-0.2.0 dask-ml-2023.3.24 multipledispatch-1.0.0 scikit-learn-1.3.0\n\u001B[43mNote: you may need to restart the kernel using dbutils.library.restartPython() to use updated packages.\u001B[0m\n"
     ]
    }
   ],
   "source": [
    "\n",
    "%pip install scikit-image\n",
    "%pip install cudf-cu11 --extra-index-url https://pypi.nvidia.com \n",
    "%pip install cuml-cu11 --extra-index-url https://pypi.nvidia.com\n",
    "%pip install dask-ml\n",
    "dbutils.library.restartPython()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f753de84-403b-4fd7-ae45-ed5c84646501",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np  \n",
    "import pandas as pd  \n",
    "import xgboost as xgb  \n",
    "from sklearn.compose import ColumnTransformer  \n",
    "from sklearn.feature_extraction.text import CountVectorizer  \n",
    "from sklearn.pipeline import Pipeline  \n",
    "from sklearn.preprocessing import StandardScaler  \n",
    "from skimage.feature import hog \n",
    "import cudf\n",
    "\n",
    "encode_dict = {}\n",
    "\n",
    "def encode_cat(x):\n",
    "    if x not in encode_dict.keys():\n",
    "        encode_dict[x] = len(encode_dict)\n",
    "    return encode_dict[x]\n",
    "\n",
    "df = pd.read_parquet('/dbfs/FileStore/tables/data_optimized_proc.parquet')\n",
    "df = df.dropna(subset=[\"IssueType\", \"screenshot\"])\n",
    "df[\"CAT\"] = df[\"IssueType\"].map(lambda x: encode_cat(x))\n",
    "print(df.groupby([\"CAT\", \"IssueType\"])[\"CAT\"].count())\n",
    "print(df.groupby([\"CAT\", \"IssueType\"])[\"CAT\"].count() / df.shape[0])\n",
    "df = cudf.from_pandas(df)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b9a62384-1266-4898-b133-ce4f15ce6123",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "train_dataset, test_dataset = train_test_split(\n",
    "    df, train_size=0.8, random_state=42, stratify=df.CAT\n",
    ")\n",
    "train_dataset = train_dataset.reset_index(drop=True)\n",
    "test_dataset = test_dataset.reset_index(drop=True)\n",
    "print(\"- TRAIN -\\n\")\n",
    "print(train_dataset.groupby([\"CAT\", \"IssueType\"])[\"CAT\"].count())\n",
    "print(\"-----------------\")\n",
    "print(\"- TEST -\\n\")\n",
    "print(test_dataset.groupby([\"CAT\", \"IssueType\"])[\"CAT\"].count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7a4ede2f-f061-453d-8797-6b7dc224c0a2",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import cuml\n",
    "def get_params():\n",
    "    learning_rate = st.uniform(0.05, 0.4)\n",
    "    max_depth = [10, 20, 50, 100, 200, 300, 500]\n",
    "    n_estimators = [5, 10, 25, 50, 75, 100, 200, 400]\n",
    "    reg_lambda = [0.1, 0.3, 1]\n",
    "    colsample_bytree = st.beta(10, 1)\n",
    "    gamma = st.uniform(0, 10)\n",
    "    subsample = [0.6, 0.8, 1.0]\n",
    "    reg_alpha = st.expon(0, 50)\n",
    "    min_child_weight = [1, 5, 10]\n",
    "    tree_method = 'gpu_hist'\n",
    "    # Create the random grid\n",
    "    return {\n",
    "        \"vectorizer__analyzer\": [\"word\"],\n",
    "        \"vectorizer__stop_words\": [\"english\"],\n",
    "        \"vectorizer__strip_accents\": [\"ascii\"],\n",
    "        \"vectorizer__ngram_range\": [(1, 1), (1, 2), (2, 2)],\n",
    "        \"classifier__n_estimators\": n_estimators,\n",
    "        \"classifier__learning_rate\": learning_rate,\n",
    "        \"classifier__reg_lambda\": reg_lambda,\n",
    "        \"classifier__colsample_bytree\": colsample_bytree,\n",
    "        \"classifier__gamma\": gamma,\n",
    "        \"classifier__subsample\": subsample,\n",
    "        \"classifier__reg_alpha\": reg_alpha,\n",
    "        \"classifier__max_depth\": max_depth,\n",
    "        \"classifier__min_child_weight\": min_child_weight,\n",
    "        \"classifier__tree_method\": tree_method\n",
    "    }\n",
    "print('RAPIDS',cuml.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "83d27bf5-2d7d-4fd9-8a01-160e82985f34",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np  \n",
    "import pandas as pd  \n",
    "from xgboost import XGBClassifier\n",
    "import xgboost\n",
    "from sklearn.compose import ColumnTransformer  \n",
    "from sklearn.feature_extraction.text import CountVectorizer  \n",
    "from sklearn.pipeline import Pipeline  \n",
    "from sklearn.preprocessing import StandardScaler  \n",
    "from skimage.feature import hog  \n",
    "import scipy.stats as st\n",
    "import mlflow\n",
    "from mlflow.tracking import MlflowClient\n",
    "import matplotlib.pyplot as plt\n",
    "from mlflow.utils.environment import _mlflow_conda_env\n",
    "from mlflow.models.signature import infer_signature\n",
    "import matplotlib\n",
    "import io\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import sklearn\n",
    "from sklearn.model_selection import (\n",
    "    RandomizedSearchCV,\n",
    "    train_test_split,\n",
    "    StratifiedKFold,\n",
    ")\n",
    "from sklearn.pipeline import Pipeline\n",
    "#from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import (\n",
    "    confusion_matrix,\n",
    "    ConfusionMatrixDisplay,\n",
    "    precision_recall_fscore_support,\n",
    "    f1_score,\n",
    "    recall_score,\n",
    "    precision_score,\n",
    ")\n",
    "import dask_ml.model_selection as dcv\n",
    "import cuml\n",
    "from cuml.feature_extraction.text import TfidfVectorizer\n",
    "import xgboost as xgb\n",
    "\n",
    "\"\"\"\n",
    "# Load image data and extract HOG features  \n",
    "hog_features = np.array([hog(np.array(df.loc[i, \"screenshot\"])) for i in range(len(df))])  \n",
    "  \n",
    "# Load text data and create feature matrix  \n",
    "vectorizer = CountVectorizer()  \n",
    "text_features = vectorizer.fit_transform(df[\"StepDescription\"]).toarray()  \n",
    "  \n",
    "# Combine image and text features  \n",
    "features = np.hstack((hog_features, text_features))  \n",
    "  \n",
    "# Create a Scikit-learn pipeline  \n",
    "preprocessor = ColumnTransformer(transformers=[  \n",
    "        (\"scaler\", StandardScaler(), slice(0, len(hog_features))),  \n",
    "        (\"nothing\", \"passthrough\", slice(len(hog_features), None))  \n",
    "    ])  \n",
    "\n",
    "model = Pipeline(steps=[  \n",
    "        (\"preprocessor\", preprocessor),  \n",
    "        (\"classifier\", xgb.XGBClassifier())  \n",
    "    ])  \n",
    "  \n",
    "# Train the model  \n",
    "X = features  \n",
    "y = image_data[\"label\"]  \n",
    "model.fit(X, y)  \n",
    "  \n",
    "# Use the model to make predictions  \n",
    "X_new = features[:10]  # Example input data  \n",
    "y_pred = model.predict(X_new)  \n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def train_tree_model(\n",
    "    train_data: pd.DataFrame, cv_params: dict, n_cv_folds: int, n_iter: int\n",
    "):\n",
    "    \"\"\"\n",
    "    :param train_data: Preprocessed dataframe that contains both the question and category\n",
    "    \"\"\"\n",
    "   # hog_features = np.array([hog(np.array(Image.open(io.BytesIO(train_data.loc[i, \"screenshot\"]))), channel_axis=-1) for i in range(len(train_data))]) \n",
    "\n",
    "    vectorizer = TfidfVectorizer()  \n",
    "    text_features = vectorizer.fit_transform(df[\"StepDescription\"]).toarray()  \n",
    "\n",
    "    #X = np.hstack((hog_features, text_features)) \n",
    "    X = train_data.StepDescription#, 'image': hog_features}\n",
    "    Y = train_data[\"CAT\"]\n",
    "\n",
    "    params = {\n",
    "        'learning_rate': 0.1,\n",
    "        'colsample_bytree' : 0.3,\n",
    "        'max_depth': 5,\n",
    "        'objective': 'reg:linear',\n",
    "        'n_estimators':10,\n",
    "        'alpha' : 10,\n",
    "        'silent': True,\n",
    "        'verbose_eval': True,\n",
    "        'tree_method':'gpu_hist',\n",
    "    }\n",
    "    dtrain = xgb.DMatrix(text_features, label=train_data[\"CAT\"])\n",
    "    classifier = xgb.train(params, dtrain, num_boost_rounds=100, evals=[(dtrain, 'train')])\n",
    "\n",
    "   # preprocessor = ColumnTransformer(transformers=[  \n",
    "   #     (\"scaler\", StandardScaler(), slice(0, len(hog_features))),  \n",
    "   #     (\"nothing\", \"passthrough\", slice(len(hog_features), None))  \n",
    "   # ])  \n",
    "\n",
    "   # preprocessor = ColumnTransformer(transformers=[  \n",
    "    #    (\"scaler\", StandardScaler(), slice(0, len(hog_features))),  \n",
    "    #    (\"tfidf\", TfidfVectorizer(), \"text\")  \n",
    "   # ]) \n",
    "    \n",
    "   # classifier = Pipeline(\n",
    "    #    steps=[\n",
    "     #       (\"vectorizer\", TfidfVectorizer()),\n",
    "           # (\"preprocessor\", TfidfVectorizer()),\n",
    "      #      (\"classifier\", XGBClassifier(random_state=10)),\n",
    "      #  ],\n",
    "      #  verbose=True,\n",
    "    #)\n",
    "   # classifier_random = dcv.RandomizedSearchCV(\n",
    "   #     classifier,\n",
    "   #     param_distributions=cv_params,\n",
    "   #     n_iter=n_iter,\n",
    "   #     verbose=0,\n",
    "   #     scoring=\"accuracy\",\n",
    "   #     random_state=42,\n",
    "   #     refit=True,\n",
    "   #     n_jobs=-1,\n",
    "   #     cv=StratifiedKFold(n_splits=n_cv_folds, shuffle=True, random_state=42),\n",
    "   # )\n",
    "   # classifier_random.fit(X, Y)\n",
    "    return classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c7dccf2c-4b39-4123-b345-2727cceb08bc",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "class SKLearnEstimatorWrapper(mlflow.pyfunc.PythonModel):\n",
    "    def __init__(self, estimator, mapping_dict):\n",
    "        self.estimator = estimator\n",
    "        self.mapping_dict = mapping_dict\n",
    "\n",
    "    def predict(self, context, model_input):\n",
    "        #image_features = np.array([hog(np.array(Image.open(io.BytesIO(model_input.loc[i, \"screenshot\"]))), channel_axis=-1) for i in range(len(model_input))]) \n",
    "\n",
    "        model_input = model_input[\"StepDescription\"]#, 'image': image_features}\n",
    "\n",
    "        outputs = self.estimator.predict(model_input)\n",
    "        ret_d = {\"class\": np.array([self.mapping_dict[str(x)] for x in outputs])}\n",
    "        return ret_d\n",
    "\n",
    "\n",
    "def log_estimator_model(estimator, encode_dict):\n",
    "    \"\"\"\n",
    "    :param estimator: An estimator that takes as input text and outputs a category (for the question)\n",
    "    :param encode_dict: A dictionary which encodes a labelled category (such as 'Guest Rooms') to a numeric category\n",
    "    \"\"\"\n",
    "    mapping_dict = {str(v): k for k, v in encode_dict.items()}\n",
    "    wrapped_estimator = SKLearnEstimatorWrapper(estimator, mapping_dict)\n",
    "    input_example = {\n",
    "        \"text\": np.array(\n",
    "            test_dataset.loc[0, 'StepDescription']\n",
    "        )\n",
    "        #'image': hog(np.array(Image.open(io.BytesIO(test_dataset.loc[0, \"screenshot\"]))), channel_axis=-1)\n",
    "    }\n",
    "    signature = infer_signature(\n",
    "        pd.DataFrame(input_example),\n",
    "        wrapped_estimator.predict(None, pd.DataFrame(input_example)),\n",
    "    )\n",
    "    # Specify the additional dependencies\n",
    "    conda_env = _mlflow_conda_env(\n",
    "        additional_conda_deps=None,\n",
    "        additional_pip_deps=[\n",
    "            \"xgboost=={}\".format(xgboost.__version__),\n",
    "            \"sklearn=={}\".format(sklearn.__version__),\n",
    "        ],\n",
    "        additional_conda_channels=None,\n",
    "    )\n",
    "    # Log model so the model artifacts can have the correct conda env, python model, and signature (data formats for input & output)\n",
    "    return mlflow.pyfunc.log_model(\n",
    "        \"Bluecumber_test_analysis\",\n",
    "        python_model=wrapped_estimator,\n",
    "        conda_env=conda_env,\n",
    "        signature=signature,\n",
    "        input_example=input_example,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a9ca17d6-8d71-46e5-80ff-0b5693646755",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "client = MlflowClient()\n",
    "n_cv_folds = 3\n",
    "\n",
    "\n",
    "def init_experiment(experiment_name):\n",
    "    # fetch existing experiments, if it doesn't exist create it and set it\n",
    "    existing_experiments = client.search_experiments()\n",
    "    if experiment_name in [exp.name for exp in existing_experiments]:\n",
    "        return mlflow.set_experiment(experiment_name)\n",
    "    else:\n",
    "        return mlflow.create_experiment(experiment_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bdecaff3-a4ec-4375-899b-90e13b0489ec",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "mlflow.end_run()\n",
    "#experiment_id = init_experiment(\n",
    "  #  \"/Users/Chris.Jose@cvent.com/test_result_analysis/experiments/#xgboost_test_result_analysis_1\")\n",
    "#run = mlflow.start_run(run_name=\"xgboost_tfidf_CV\", #experiment_id=experiment_id)\n",
    "\n",
    "experiment_name = \"/Users/Chris.Jose@cvent.com/test_result_analysis/experiments/#xgboost_test_result_analysis_1\"\n",
    "mlflow.set_experiment(experiment_name)  \n",
    "  \n",
    "# Get the experiment ID  \n",
    "experiment = mlflow.get_experiment_by_name(experiment_name)  \n",
    "experiment_id = experiment.experiment_id  \n",
    "  \n",
    "# Start a new MLflow run  \n",
    "run = mlflow.start_run(run_name=\"xgboost_tfidf_CV\", experiment_id=experiment_id)\n",
    "\n",
    "\n",
    "mlflow.sklearn.autolog(log_models=False)\n",
    "mlflow.log_param(\"n_cv_folds\", 3)\n",
    "classifier_random = train_tree_model(\n",
    "    train_dataset, get_params(), n_cv_folds, n_iter=30000\n",
    ")\n",
    "print(classifier_random.best_score_)\n",
    "#test_image_features = np.array([hog(np.array(Image.open(io.BytesIO(test_dataset.loc[i, \"screenshot\"]))), channel_axis=-1) for i in range(len(test_dataset))]) \n",
    "\n",
    "#model_input = {'text': test_dataset[\"StepDescription\"], 'image': test_image_features}\n",
    "X_test = test_dataset[\"StepDescription\"]\n",
    "Y_test = test_dataset[\"CAT\"]\n",
    "mlflow.log_metric(\n",
    "    \"testing_accuracy_score\", classifier_random.best_estimator_.score(X_test, Y_test)\n",
    ")\n",
    "estimator = classifier_random.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "dd548d8e-5e28-4b32-bdda-f3fda7e74caf",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "y_pred = classifier_random.best_estimator_.predict(X_test)\n",
    "y_true = Y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6c9f3eaa-d312-4539-a03a-f2af099b00f5",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "font = {\"family\": \"normal\", \"size\": 15}\n",
    "\n",
    "matplotlib.rc(\"font\", **font)\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "fig, ax = plt.subplots()\n",
    "sns.heatmap(\n",
    "    cm,\n",
    "    annot=True,\n",
    "    annot_kws={\"size\": 12},\n",
    "    ax=ax,\n",
    "    cmap=\"Blues\",\n",
    "    fmt=\"g\",\n",
    "    xticklabels=list(encode_dict.keys()),\n",
    "    yticklabels=list(encode_dict.keys()),\n",
    ")\n",
    "plt.xticks(rotation=45, horizontalalignment=\"right\")\n",
    "fig.set_size_inches(12, 12)\n",
    "plt.title(\"Confusion Matrix - Test Set\", fontdict={\"family\": \"normal\", \"size\": 21})\n",
    "img_buf = io.BytesIO()\n",
    "plt.ylabel(\"True label\", fontdict={\"family\": \"normal\", \"size\": 21})\n",
    "plt.xlabel(\"Predicted label\", fontdict={\"family\": \"normal\", \"size\": 21})\n",
    "plt.show()\n",
    "fig.savefig(img_buf, dpi=fig.dpi, bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bec5a19b-a05f-4a48-818c-03a7bfe99720",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "font = {\"family\": \"normal\", \"size\": 15}\n",
    "\n",
    "matplotlib.rc(\"font\", **font)\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "fig, ax = plt.subplots()\n",
    "sns.heatmap(\n",
    "    cm / cm.sum(axis=1).reshape(-1, 1),\n",
    "    annot=True,\n",
    "    annot_kws={\"size\": 12},\n",
    "    ax=ax,\n",
    "    cmap=\"Blues\",\n",
    "    fmt=\".2%\",\n",
    "    xticklabels=list(encode_dict.keys()),\n",
    "    yticklabels=list(encode_dict.keys()),\n",
    ")\n",
    "plt.xticks(rotation=45, horizontalalignment=\"right\")\n",
    "fig.set_size_inches(12, 12)\n",
    "plt.title(\n",
    "    \"Confusion Matrix - Test Set Percentage\", fontdict={\"family\": \"normal\", \"size\": 21}\n",
    ")\n",
    "plt.ylabel(\"True label\", fontdict={\"family\": \"normal\", \"size\": 21})\n",
    "plt.xlabel(\"Predicted label\", fontdict={\"family\": \"normal\", \"size\": 21})\n",
    "img_buf2 = io.BytesIO()\n",
    "plt.show()\n",
    "fig.savefig(img_buf2, dpi=fig.dpi, bbox_inches=\"tight\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "xgboost-experiment",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
