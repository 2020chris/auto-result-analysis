base:
  use_cuda: True

data:
  data_dir: /dbfs/mnt/s3_mount/qe_automation/data
 # data_dir: /dbfs/FileStore/shared_uploads/Chris.Jose@cvent.com
  images_folder: images
  full_dataset: merged_five_months.parquet
#  full_dataset: subset.csv 
  train_dataset: train_dataset.csv
  test_dataset: test_dataset.csv
  description_col: StepDescription
  image_col: image_id
  answer_col: IssueType
  num_labels: 14
  train_size: 


tokenizer:
  padding: True
  max_length: 512
  truncation: True
  add_special_tokens: True
  return_token_type_ids: True
  return_attention_mask: True


model:
  name: roberta-beit  # Custom name for the multimodal model
  text_encoder: distilbert-base-uncased #vives/distilbert-base-uncased-finetuned-cvent-2019_2022
  image_encoder: microsoft/beit-base-patch16-224-pt22k-ft22k  
  intermediate_dims: 512
  dropout: 0.5

train:
  output_dir: mlflow_checkpoint
  path: /Users/Chris.Jose@cvent.com/test_result_analysis/experiments/
  checkpoints: /dbfs/mnt/s3_mount/qe_automation/checkpoints/
  model_repo: /dbfs/mnt/s3_mount/qe_automation/model_repository/
  resume: /dbfs/mnt/s3_mount/qe_automation/checkpoints/model_best.pth.tar
  seed: 12345
  num_train_epochs: 50
  learning_rate: 5.0e-5
  weight_decay: 0.0
  warmup_ratio: 0.0
  warmup_steps: 0
  evaluation_strategy: steps
  eval_steps: 100
  log_interval: 10
  logging_strategy: steps
  logging_steps: 100
  save_strategy: steps
  save_steps: 100
  save_total_limit: 3            # Save only the last 3 checkpoints at any given time while training 
  metric_for_best_model: wups
  per_device_train_batch_size: 128
  per_device_eval_batch_size: 128
  remove_unused_columns: False
  dataloader_num_workers: 8
  load_best_model_at_end: True

metrics:
  metrics_folder: /dbfs/FileStore/shared_uploads/Chris.Jose@cvent.com/metrics
  metrics_file: metrics.json

inference:
  checkpoint: checkpoint-1500
